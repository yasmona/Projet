{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "jewish-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Input,ConvLSTM2D,Reshape,Activation,Lambda,Softmax,Concatenate\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, TimeDistributed, Flatten, Dropout,RepeatVector,Reshape,Activation,Lambda,Softmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "import keras.backend as kerback\n",
    "#from keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "killing-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import random\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "complete-checkout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(f):\n",
    "    data = pd.read_csv(f, sep=\"\\n\",header=None)\n",
    "    list_listword=[]#liste des listes des mots de chaque tweet\n",
    "    list_tweets=data[0].values.tolist()#liste des tweets\n",
    "    l=[]\n",
    "    for text in list_tweets:\n",
    "        text = re.sub(r'http\\S+', '', text)   # Remove URLs\n",
    "        text = re.sub(r'â€”', ' ', text) \n",
    "        text = re.sub(r'@[a-zA-Z0-9_]+', '', text)  # Remove @ mentions\n",
    "        text = text.strip(\" \")   # Remove whitespace resulting from above\n",
    "        text = re.sub(r' +', ' ', text)   # Remove redundant spaces\n",
    "        l.append(text)\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "original-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tweets=prepare_data('C:/Users/LENOVO/Desktop/PCD/tweet.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "false-richards",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66,)\n",
      "841\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_data(list_tweets):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(list_tweets)\n",
    "    sequences = tokenizer.texts_to_sequences(list_tweets)\n",
    "    vocab_size=len(tokenizer.word_index)\n",
    "    return sequences, tokenizer, vocab_size\n",
    "\n",
    "def max_tweet(sequences):\n",
    "    max_length=0\n",
    "    for i in range(len(sequences)):\n",
    "        if(len(sequences[i])>max_length):\n",
    "            max_length=len(sequences[i])\n",
    "    return max_length\n",
    "\n",
    "sequences, tokenizer, vocab_size=tokenizer_data(list_tweets)\n",
    "max_length=max_tweet(sequences)\n",
    "sequences=np.array([xi+[0]*(max_length-len(xi)) for xi in sequences])\n",
    "y=sequences[:,-1]\n",
    "print(y.shape)\n",
    "y_train = y.reshape(-1, 1)\n",
    "X=sequences\n",
    "X_train = np.expand_dims(X, axis=2)\n",
    "nb_tweet=len(sequences)\n",
    "batch_size=max_length\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepting-civilian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 57, 1)        841         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 57, 2)        0           input_1[0][0]                    \n",
      "                                                                 embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 15)           1080        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 57, 15)       0           lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 57, 15)       1860        repeat_vector[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 57, 1)        16          lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 [(None, 57)]         0           time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 57)           3306        lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 57, 1)        0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,103\n",
      "Trainable params: 7,103\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1e971cf5af0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " import keras.backend as kerback\n",
    "# define the standalone generator model\n",
    "def build_generator(n_outputs):#max_length nombre d'element das une tweet\n",
    "   \n",
    "    noise=Input(shape=(57,1))\n",
    "    \n",
    "    label=Input(shape=(57,))\n",
    "    label_emb=Embedding(vocab_size,1)(label)\n",
    "    \n",
    "    x0=Concatenate(axis=2)([noise,label_emb])\n",
    "    x1=LSTM(15, activation='relu',input_dim=n_outputs)(x0)\n",
    "    #15 nombre de neurone ,batch_size=n_batch=20 \n",
    "    # repeat vector\n",
    "    x2=RepeatVector(n_outputs)(x1)\n",
    "    # decoder layer\n",
    "    x3=LSTM(15, activation='relu', return_sequences=True)(x2)\n",
    "    x4=TimeDistributed(Dense(1))(x3)\n",
    "    unstacked = Lambda(lambda x: tensorflow.unstack(x, axis=2))(x4)\n",
    "    dense_outputs = [Dense(n_outputs)(x) for x in unstacked]\n",
    "    merged = Lambda(lambda x: K.stack(x, axis=2))(dense_outputs)\n",
    "\n",
    "    #generated_tweet=model(x0)\n",
    "    model = Model([noise,label],merged)\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    print('generator')\n",
    "    print(model.summary())\n",
    "    return  model\n",
    "\n",
    "build_generator(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "level-strap",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 57, 1)        841         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 114, 1)       0           input_3[0][0]                    \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 15)           1020        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 57, 15)       0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 57, 15)       1860        repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 57, 1)        16          lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               [(None, 57)]         0           time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 57)           3306        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 57, 1)        0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 57)           0           lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            58          flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,101\n",
      "Trainable params: 7,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1e971e2eb50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the standalone discriminator model\n",
    "def build_discriminator(n_inputs):\n",
    "    tweet=Input(shape=(n_inputs,1)) #n_imput nombre des element \n",
    "    \n",
    "    label=Input(shape=(n_inputs,))\n",
    "    label_emb=Embedding(vocab_size,1)(label)\n",
    "    \n",
    "    x0=Concatenate(axis=1)([tweet,label_emb])\n",
    "    x2=LSTM(15, activation='relu', input_dim=n_inputs)(x0)\n",
    "    x3=RepeatVector(n_inputs)(x2)\n",
    "    x4=LSTM(15, activation='relu', return_sequences=True)(x3)\n",
    "    x5=TimeDistributed(Dense(1))(x4)\n",
    "    unstacked = Lambda(lambda x: tensorflow.unstack(x, axis=2))(x5)\n",
    "    dense_outputs = [Dense(n_inputs)(x) for x in unstacked]\n",
    "    merged = Lambda(lambda x: K.stack(x, axis=2))(dense_outputs)\n",
    "    x1=Flatten()(merged)\n",
    "    x6=Dense(1, activation='sigmoid')(x1)\n",
    "    model = Model([tweet,label], x6)\n",
    "    print(model.summary())\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy' ,metrics=['accuracy'])\n",
    "    return model\n",
    "build_discriminator(57)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collectible-pastor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 57, 1)        841         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 57, 2)        0           input_5[0][0]                    \n",
      "                                                                 embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 15)           1080        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_2 (RepeatVector)  (None, 57, 15)       0           lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 57, 15)       1860        repeat_vector_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 57, 1)        16          lstm_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               [(None, 57)]         0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 57)           3306        lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 57, 1)        0           dense_6[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 7,103\n",
      "Trainable params: 7,103\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"functional_7\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 57, 1)        841         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 114, 1)       0           input_7[0][0]                    \n",
      "                                                                 embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 15)           1020        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_3 (RepeatVector)  (None, 57, 15)       0           lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_7 (LSTM)                   (None, 57, 15)       1860        repeat_vector_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 57, 1)        16          lstm_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               [(None, 57)]         0           time_distributed_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 57)           3306        lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 57, 1)        0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 57)           0           lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            58          flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,101\n",
      "Trainable params: 7,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(57, 1)\n",
      "(None, 1)\n",
      "Model: \"functional_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_5 (Functional)       (None, 57, 1)        7103        input_9[0][0]                    \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "functional_7 (Functional)       (None, 1)            7101        functional_5[0][0]               \n",
      "                                                                 input_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,204\n",
      "Trainable params: 7,103\n",
      "Non-trainable params: 7,101\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.functional.Functional at 0x1e972158730>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the combined generator and discriminator model\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    #model.add(generator)\n",
    "    x0=Input(shape=(57,1))\n",
    "    x1=Input(shape=(57,))\n",
    "    x2=generator([x0,x1])\n",
    "    print(x2[-1].shape)\n",
    "    x3=discriminator([x2,x1])\n",
    "    print(x3.shape)\n",
    "    model = Model([x0,x1], x3)\n",
    "    # add the discriminator\n",
    "    #model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "define_gan(build_generator(57), build_discriminator(57))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "parliamentary-compatibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[146],\n",
       "         [  7],\n",
       "         [ 46],\n",
       "         ...,\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[ 87],\n",
       "         [ 12],\n",
       "         [  4],\n",
       "         ...,\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[ 91],\n",
       "         [ 59],\n",
       "         [ 12],\n",
       "         ...,\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 68],\n",
       "         [  3],\n",
       "         [320],\n",
       "         ...,\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[ 25],\n",
       "         [  8],\n",
       "         [ 24],\n",
       "         ...,\n",
       "         [  0],\n",
       "         [  0],\n",
       "         [  0]],\n",
       " \n",
       "        [[834],\n",
       "         [305],\n",
       "         [  1],\n",
       "         ...,\n",
       "         [  6],\n",
       "         [ 61],\n",
       "         [ 62]]]),\n",
       " array([[ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [ 0],\n",
       "        [62]])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_real_samples():\n",
    "    list_tweets=prepare_data('C:/Users/LENOVO/Desktop/PCD/tweet.txt')\n",
    "    sequences, tokenizer, vocab_size=tokenizer_data(list_tweets)\n",
    "    max_length=max_tweet(sequences)\n",
    "    sequences=np.array([xi+[0]*(max_length-len(xi)) for xi in sequences])\n",
    "    y=sequences[:,-1]\n",
    "    #y_train = y.reshape(-1, 1)\n",
    "    X=sequences\n",
    "    X_train = np.expand_dims(X, axis=2)\n",
    "    #print(X_train.shape)\n",
    "    nb_tweet=len(sequences)\n",
    "    batch_size=max_length\n",
    "    \n",
    "    return [X_train,y_train]\n",
    "generate_real_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "general-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch labels to use when calling train_on_batch\n",
    "valid = ones((max_length,1))\n",
    "fake = zeros((max_length,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opened-charm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "Model: \"functional_11\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 57, 1)        841         input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 57, 2)        0           input_11[0][0]                   \n",
      "                                                                 embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 15)           1080        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 57, 15)       0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 57, 15)       1860        repeat_vector_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 57, 1)        16          lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               [(None, 57)]         0           time_distributed_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 57)           3306        lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 57, 1)        0           dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 7,103\n",
      "Trainable params: 7,103\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 57, 1)        841         input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 114, 1)       0           input_13[0][0]                   \n",
      "                                                                 embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 15)           1020        concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_5 (RepeatVector)  (None, 57, 15)       0           lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 57, 15)       1860        repeat_vector_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_5 (TimeDistrib (None, 57, 1)        16          lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              [(None, 57)]         0           time_distributed_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 57)           3306        lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 57, 1)        0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 57)           0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            58          flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,101\n",
      "Trainable params: 7,101\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(57, 1)\n",
      "(None, 1)\n",
      "Model: \"functional_15\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 57, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_16 (InputLayer)           [(None, 57)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "functional_11 (Functional)      (None, 57, 1)        7103        input_15[0][0]                   \n",
      "                                                                 input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "functional_13 (Functional)      (None, 1)            7101        functional_11[0][0]              \n",
      "                                                                 input_16[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 14,204\n",
      "Trainable params: 7,103\n",
      "Non-trainable params: 7,101\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator(max_length)\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(max_length)\n",
    "#discriminator.compile(optimizer='adam', loss='binary_crossentropy' ,metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the combined model\n",
    "gan_model = define_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "anonymous-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tweet(epoch,noise,sampled_labels,g_model):\n",
    "    result = g_model.predict([noise,sampled_labels])\n",
    "    #print(result)\n",
    "    #print(result.shape)\n",
    "    #print(len(result))\n",
    "    res=[]\n",
    "    predicted_word =\"\"\n",
    "    for j in range(57):\n",
    "        #predicted_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == int(result[0][j]):\n",
    "                predicted_word = predicted_word+' '+word\n",
    "                break\n",
    "    res.append(predicted_word)\n",
    "    print(res)\n",
    "    \n",
    "    #for i in res:\n",
    "    #    print(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "oriented-arbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_text(model_disc, seed_text):\n",
    "    #print(seed_text)\n",
    "    sequences, tokenizer=tokenizer_data(seed_text)\n",
    "    sequences= pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    y = model_disc.predict(sequences)\n",
    "    #print(y)\n",
    "    #for i in y:\n",
    "    if(i<0.7):\n",
    "        print(\"fake\")\n",
    "    else:\n",
    "        print(\"real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-nebraska",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 57, 1) for input Tensor(\"input_11:0\", shape=(None, 57, 1), dtype=float32), but it was called on an input with incompatible shape (None, 1, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_12:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_14:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_14:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_14:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57, 1) for input Tensor(\"input_15:0\", shape=(None, 57, 1), dtype=float32), but it was called on an input with incompatible shape (57, 1, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_16:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57, 1) for input Tensor(\"input_11:0\", shape=(None, 57, 1), dtype=float32), but it was called on an input with incompatible shape (57, 1, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_12:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_14:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57, 1) for input Tensor(\"input_15:0\", shape=(None, 57, 1), dtype=float32), but it was called on an input with incompatible shape (57, 1, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_16:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57, 1) for input Tensor(\"input_11:0\", shape=(None, 57, 1), dtype=float32), but it was called on an input with incompatible shape (57, 1, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_12:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_14:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (57, 1).\n",
      "epoch: 1, d_loss: 0.69,         d_acc: 0.21, g_loss: 0.69\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57, 1) for input Tensor(\"input_11:0\", shape=(None, 57, 1), dtype=float32), but it was called on an input with incompatible shape (None, 1, 1).\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 57) for input Tensor(\"input_12:0\", shape=(None, 57), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "['']\n",
      "epoch: 2, d_loss: 0.66,         d_acc: 0.47, g_loss: 0.68\n",
      "['']\n",
      "epoch: 3, d_loss: 0.61,         d_acc: 0.50, g_loss: 0.68\n",
      "['']\n",
      "epoch: 4, d_loss: 0.59,         d_acc: 0.50, g_loss: 0.68\n",
      "['']\n",
      "epoch: 5, d_loss: 0.59,         d_acc: 0.48, g_loss: 0.69\n",
      "['']\n",
      "epoch: 6, d_loss: 0.55,         d_acc: 0.50, g_loss: 0.69\n",
      "['']\n",
      "epoch: 7, d_loss: 0.49,         d_acc: 0.50, g_loss: 0.69\n",
      "['']\n",
      "epoch: 8, d_loss: 0.49,         d_acc: 0.50, g_loss: 0.70\n",
      "['']\n",
      "epoch: 9, d_loss: 0.50,         d_acc: 1.00, g_loss: 0.70\n",
      "['']\n",
      "epoch: 10, d_loss: 0.45,         d_acc: 0.98, g_loss: 0.71\n",
      "['']\n",
      "epoch: 11, d_loss: 0.44,         d_acc: 0.98, g_loss: 0.71\n",
      "['']\n",
      "epoch: 12, d_loss: 0.45,         d_acc: 0.97, g_loss: 0.72\n",
      "['']\n",
      "epoch: 13, d_loss: 0.43,         d_acc: 0.97, g_loss: 0.73\n",
      "['']\n",
      "epoch: 14, d_loss: 0.42,         d_acc: 0.98, g_loss: 0.74\n",
      "['']\n",
      "epoch: 15, d_loss: 0.37,         d_acc: 0.98, g_loss: 0.75\n",
      "['']\n",
      "epoch: 16, d_loss: 0.39,         d_acc: 0.96, g_loss: 0.76\n",
      "['']\n",
      "epoch: 17, d_loss: 0.36,         d_acc: 0.98, g_loss: 0.78\n",
      "['']\n",
      "epoch: 18, d_loss: 0.36,         d_acc: 0.96, g_loss: 0.79\n",
      "['']\n",
      "epoch: 19, d_loss: 0.36,         d_acc: 0.96, g_loss: 0.80\n",
      "['']\n",
      "epoch: 20, d_loss: 0.31,         d_acc: 0.99, g_loss: 0.82\n",
      "['']\n",
      "epoch: 21, d_loss: 0.32,         d_acc: 0.98, g_loss: 0.84\n",
      "['']\n",
      "epoch: 22, d_loss: 1.39,         d_acc: 0.98, g_loss: 0.85\n",
      "['']\n",
      "epoch: 23, d_loss: 0.31,         d_acc: 0.98, g_loss: 0.87\n",
      "['']\n",
      "epoch: 24, d_loss: 0.29,         d_acc: 1.00, g_loss: 0.88\n",
      "['']\n",
      "epoch: 25, d_loss: 0.28,         d_acc: 1.00, g_loss: 0.90\n",
      "['']\n",
      "epoch: 26, d_loss: 0.27,         d_acc: 1.00, g_loss: 0.91\n",
      "['']\n",
      "epoch: 27, d_loss: 0.29,         d_acc: 0.98, g_loss: 0.93\n",
      "['']\n",
      "epoch: 28, d_loss: 0.29,         d_acc: 0.96, g_loss: 0.95\n",
      "['']\n",
      "epoch: 29, d_loss: 0.27,         d_acc: 0.96, g_loss: 0.97\n",
      "['']\n",
      "epoch: 30, d_loss: 0.27,         d_acc: 0.96, g_loss: 0.99\n",
      "['']\n",
      "epoch: 31, d_loss: 0.24,         d_acc: 1.00, g_loss: 1.01\n",
      "['']\n",
      "epoch: 32, d_loss: 0.24,         d_acc: 0.98, g_loss: 1.04\n",
      "['']\n",
      "epoch: 33, d_loss: 0.25,         d_acc: 0.96, g_loss: 1.06\n",
      "['']\n",
      "epoch: 34, d_loss: 0.23,         d_acc: 0.98, g_loss: 1.08\n",
      "['']\n",
      "epoch: 35, d_loss: 0.25,         d_acc: 0.96, g_loss: 1.11\n",
      "['']\n",
      "epoch: 36, d_loss: 0.22,         d_acc: 0.98, g_loss: 1.13\n",
      "['']\n",
      "epoch: 37, d_loss: 0.21,         d_acc: 0.99, g_loss: 1.15\n",
      "['']\n",
      "epoch: 38, d_loss: 0.22,         d_acc: 0.96, g_loss: 1.18\n",
      "['']\n",
      "epoch: 39, d_loss: 0.22,         d_acc: 0.96, g_loss: 1.20\n",
      "['']\n",
      "epoch: 40, d_loss: 0.20,         d_acc: 0.98, g_loss: 1.22\n",
      "['']\n",
      "epoch: 41, d_loss: 0.19,         d_acc: 0.98, g_loss: 1.25\n",
      "['']\n",
      "epoch: 42, d_loss: 0.21,         d_acc: 0.96, g_loss: 1.27\n",
      "['']\n",
      "epoch: 43, d_loss: 0.19,         d_acc: 0.98, g_loss: 1.30\n",
      "['']\n",
      "epoch: 44, d_loss: 0.17,         d_acc: 0.99, g_loss: 1.32\n",
      "['']\n",
      "epoch: 45, d_loss: 0.16,         d_acc: 1.00, g_loss: 1.35\n",
      "['']\n",
      "epoch: 46, d_loss: 0.15,         d_acc: 1.00, g_loss: 1.38\n",
      "['']\n",
      "epoch: 47, d_loss: 0.15,         d_acc: 0.99, g_loss: 1.40\n",
      "['']\n",
      "epoch: 48, d_loss: 0.16,         d_acc: 0.98, g_loss: 1.43\n",
      "['']\n",
      "epoch: 49, d_loss: 0.17,         d_acc: 0.97, g_loss: 1.46\n",
      "['']\n",
      "epoch: 50, d_loss: 0.14,         d_acc: 0.99, g_loss: 1.48\n",
      "['']\n",
      "epoch: 51, d_loss: 0.14,         d_acc: 0.99, g_loss: 1.52\n",
      "['']\n",
      "epoch: 52, d_loss: 0.12,         d_acc: 1.00, g_loss: 1.55\n",
      "['']\n",
      "epoch: 53, d_loss: 0.12,         d_acc: 1.00, g_loss: 1.58\n",
      "['']\n",
      "epoch: 54, d_loss: 0.14,         d_acc: 0.98, g_loss: 1.61\n",
      "['']\n",
      "epoch: 55, d_loss: 0.14,         d_acc: 0.98, g_loss: 1.64\n",
      "['']\n",
      "epoch: 56, d_loss: 0.18,         d_acc: 0.96, g_loss: 1.67\n",
      "['']\n",
      "epoch: 57, d_loss: 0.13,         d_acc: 0.98, g_loss: 1.70\n",
      "['']\n",
      "epoch: 58, d_loss: 0.15,         d_acc: 0.97, g_loss: 1.73\n",
      "['']\n",
      "epoch: 59, d_loss: 0.14,         d_acc: 0.97, g_loss: 1.76\n",
      "['']\n",
      "epoch: 60, d_loss: 0.12,         d_acc: 0.98, g_loss: 1.79\n",
      "['']\n",
      "epoch: 61, d_loss: 0.13,         d_acc: 0.97, g_loss: 1.82\n",
      "['']\n",
      "epoch: 62, d_loss: 0.19,         d_acc: 0.94, g_loss: 1.85\n",
      "['']\n",
      "epoch: 63, d_loss: 0.13,         d_acc: 0.96, g_loss: 1.88\n",
      "['']\n",
      "epoch: 64, d_loss: 0.14,         d_acc: 0.96, g_loss: 1.90\n",
      "['']\n",
      "epoch: 65, d_loss: 0.10,         d_acc: 0.99, g_loss: 1.93\n",
      "['']\n",
      "epoch: 66, d_loss: 0.10,         d_acc: 0.98, g_loss: 1.97\n",
      "['']\n",
      "epoch: 67, d_loss: 0.08,         d_acc: 0.99, g_loss: 2.00\n",
      "['']\n",
      "epoch: 68, d_loss: 0.10,         d_acc: 0.98, g_loss: 2.03\n",
      "['']\n",
      "epoch: 69, d_loss: 0.10,         d_acc: 0.98, g_loss: 2.06\n",
      "['']\n",
      "epoch: 70, d_loss: 0.12,         d_acc: 0.97, g_loss: 2.09\n",
      "['']\n",
      "epoch: 71, d_loss: 0.10,         d_acc: 0.98, g_loss: 2.13\n",
      "['']\n",
      "epoch: 72, d_loss: 0.08,         d_acc: 0.99, g_loss: 2.16\n",
      "['']\n",
      "epoch: 73, d_loss: 0.15,         d_acc: 0.96, g_loss: 2.19\n",
      "['']\n",
      "epoch: 74, d_loss: 0.10,         d_acc: 0.96, g_loss: 2.22\n",
      "['']\n",
      "epoch: 75, d_loss: 0.07,         d_acc: 0.98, g_loss: 2.24\n",
      "['']\n",
      "epoch: 76, d_loss: 0.09,         d_acc: 0.98, g_loss: 2.27\n",
      "['']\n",
      "epoch: 77, d_loss: 0.08,         d_acc: 0.99, g_loss: 2.30\n",
      "['']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 78, d_loss: 0.06,         d_acc: 1.00, g_loss: 2.33\n",
      "['']\n",
      "epoch: 79, d_loss: 0.10,         d_acc: 0.97, g_loss: 2.36\n",
      "['']\n",
      "epoch: 80, d_loss: 0.09,         d_acc: 0.98, g_loss: 2.39\n",
      "['']\n",
      "epoch: 81, d_loss: 0.12,         d_acc: 0.97, g_loss: 2.41\n",
      "['']\n",
      "epoch: 82, d_loss: 0.05,         d_acc: 1.00, g_loss: 2.44\n",
      "['']\n",
      "epoch: 83, d_loss: 0.07,         d_acc: 0.99, g_loss: 2.48\n",
      "['']\n",
      "epoch: 84, d_loss: 0.11,         d_acc: 0.97, g_loss: 2.51\n",
      "['']\n",
      "epoch: 85, d_loss: 0.06,         d_acc: 0.99, g_loss: 2.54\n",
      "['']\n",
      "epoch: 86, d_loss: 0.08,         d_acc: 0.98, g_loss: 2.57\n",
      "['']\n",
      "epoch: 87, d_loss: 0.09,         d_acc: 0.98, g_loss: 2.60\n",
      "['']\n",
      "epoch: 88, d_loss: 510035.57,         d_acc: 0.57, g_loss: 2.80\n",
      "['']\n",
      "epoch: 89, d_loss: 0.05,         d_acc: 0.99, g_loss: 2.97\n",
      "['']\n",
      "epoch: 90, d_loss: 0.12,         d_acc: 0.96, g_loss: 3.10\n",
      "['']\n",
      "epoch: 91, d_loss: 0.16,         d_acc: 0.96, g_loss: 3.21\n",
      "['']\n",
      "epoch: 92, d_loss: 0.24,         d_acc: 0.91, g_loss: 3.27\n",
      "['']\n",
      "epoch: 93, d_loss: 0.41,         d_acc: 0.83, g_loss: 3.30\n",
      "['']\n",
      "epoch: 94, d_loss: 0.70,         d_acc: 0.72, g_loss: 3.28\n",
      "['']\n",
      "epoch: 95, d_loss: 0.62,         d_acc: 0.75, g_loss: 3.23\n",
      "['']\n",
      "epoch: 96, d_loss: 0.61,         d_acc: 0.79, g_loss: 3.15\n",
      "['']\n",
      "epoch: 97, d_loss: 0.74,         d_acc: 0.70, g_loss: 3.06\n",
      "['']\n",
      "epoch: 98, d_loss: 0.78,         d_acc: 0.70, g_loss: 2.96\n",
      "['']\n",
      "epoch: 99, d_loss: 0.78,         d_acc: 0.70, g_loss: 2.85\n",
      "['']\n",
      "epoch: 100, d_loss: 0.58,         d_acc: 0.78, g_loss: 2.75\n",
      "['']\n",
      "epoch: 101, d_loss: 0.76,         d_acc: 0.71, g_loss: 2.64\n",
      "['']\n",
      "epoch: 102, d_loss: 0.70,         d_acc: 0.72, g_loss: 2.53\n",
      "['']\n",
      "epoch: 103, d_loss: 0.66,         d_acc: 0.73, g_loss: 2.43\n",
      "['']\n",
      "epoch: 104, d_loss: 0.84,         d_acc: 0.62, g_loss: 2.33\n",
      "['']\n",
      "epoch: 105, d_loss: 0.68,         d_acc: 0.70, g_loss: 2.23\n",
      "['']\n",
      "epoch: 106, d_loss: 0.67,         d_acc: 0.69, g_loss: 2.13\n",
      "['']\n",
      "epoch: 107, d_loss: 0.74,         d_acc: 0.63, g_loss: 2.04\n",
      "['']\n",
      "epoch: 108, d_loss: 0.63,         d_acc: 0.70, g_loss: 1.95\n",
      "['']\n",
      "epoch: 109, d_loss: 0.59,         d_acc: 0.70, g_loss: 1.87\n",
      "['']\n",
      "epoch: 110, d_loss: 0.63,         d_acc: 0.68, g_loss: 1.79\n",
      "['']\n",
      "epoch: 111, d_loss: 0.56,         d_acc: 0.72, g_loss: 1.72\n",
      "['']\n",
      "epoch: 112, d_loss: 0.54,         d_acc: 0.70, g_loss: 1.66\n",
      "['']\n",
      "epoch: 113, d_loss: 0.57,         d_acc: 0.69, g_loss: 1.60\n",
      "['']\n",
      "epoch: 114, d_loss: 0.53,         d_acc: 0.72, g_loss: 1.55\n",
      "['']\n",
      "epoch: 115, d_loss: 0.58,         d_acc: 0.66, g_loss: 1.50\n",
      "['']\n",
      "epoch: 116, d_loss: 0.59,         d_acc: 0.68, g_loss: 1.45\n",
      "['']\n",
      "epoch: 117, d_loss: 0.48,         d_acc: 0.74, g_loss: 1.41\n",
      "['']\n",
      "epoch: 118, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.37\n",
      "['']\n",
      "epoch: 119, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.34\n",
      "['']\n",
      "epoch: 120, d_loss: 0.47,         d_acc: 0.74, g_loss: 1.31\n",
      "['']\n",
      "epoch: 121, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.29\n",
      "['']\n",
      "epoch: 122, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.27\n",
      "['']\n",
      "epoch: 123, d_loss: 0.48,         d_acc: 0.74, g_loss: 1.25\n",
      "['']\n",
      "epoch: 124, d_loss: 0.48,         d_acc: 0.74, g_loss: 1.23\n",
      "['']\n",
      "epoch: 125, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.21\n",
      "['']\n",
      "epoch: 126, d_loss: 0.55,         d_acc: 0.68, g_loss: 1.20\n",
      "['']\n",
      "epoch: 127, d_loss: 0.53,         d_acc: 0.68, g_loss: 1.18\n",
      "['']\n",
      "epoch: 128, d_loss: 0.45,         d_acc: 0.75, g_loss: 1.17\n",
      "['']\n",
      "epoch: 129, d_loss: 0.51,         d_acc: 0.73, g_loss: 1.16\n",
      "['']\n",
      "epoch: 130, d_loss: 0.46,         d_acc: 0.75, g_loss: 1.15\n",
      "['']\n",
      "epoch: 131, d_loss: 0.53,         d_acc: 0.70, g_loss: 1.14\n",
      "['']\n",
      "epoch: 132, d_loss: 0.46,         d_acc: 0.75, g_loss: 1.13\n",
      "['']\n",
      "epoch: 133, d_loss: 0.46,         d_acc: 0.75, g_loss: 1.13\n",
      "['']\n",
      "epoch: 134, d_loss: 0.49,         d_acc: 0.75, g_loss: 1.12\n",
      "['']\n",
      "epoch: 135, d_loss: 0.55,         d_acc: 0.69, g_loss: 1.12\n",
      "['']\n",
      "epoch: 136, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.11\n",
      "['']\n",
      "epoch: 137, d_loss: 0.51,         d_acc: 0.74, g_loss: 1.11\n",
      "['']\n",
      "epoch: 138, d_loss: 0.49,         d_acc: 0.77, g_loss: 1.11\n",
      "['']\n",
      "epoch: 139, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.10\n",
      "['']\n",
      "epoch: 140, d_loss: 0.50,         d_acc: 0.76, g_loss: 1.10\n",
      "['']\n",
      "epoch: 141, d_loss: 0.49,         d_acc: 0.77, g_loss: 1.10\n",
      "['']\n",
      "epoch: 142, d_loss: 0.53,         d_acc: 0.70, g_loss: 1.10\n",
      "['']\n",
      "epoch: 143, d_loss: 0.49,         d_acc: 0.74, g_loss: 1.09\n",
      "['']\n",
      "epoch: 144, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.09\n",
      "['']\n",
      "epoch: 145, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.09\n",
      "['']\n",
      "epoch: 146, d_loss: 0.52,         d_acc: 0.73, g_loss: 1.09\n",
      "['']\n",
      "epoch: 147, d_loss: 0.49,         d_acc: 0.76, g_loss: 1.09\n",
      "['']\n",
      "epoch: 148, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.09\n",
      "['']\n",
      "epoch: 149, d_loss: 0.40,         d_acc: 0.84, g_loss: 1.09\n",
      "['']\n",
      "epoch: 150, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.09\n",
      "['']\n",
      "epoch: 151, d_loss: 0.54,         d_acc: 0.72, g_loss: 1.09\n",
      "['']\n",
      "epoch: 152, d_loss: 0.52,         d_acc: 0.71, g_loss: 1.09\n",
      "['']\n",
      "epoch: 153, d_loss: 0.46,         d_acc: 0.76, g_loss: 1.09\n",
      "['']\n",
      "epoch: 154, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.09\n",
      "['']\n",
      "epoch: 155, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.09\n",
      "['']\n",
      "epoch: 156, d_loss: 0.45,         d_acc: 0.78, g_loss: 1.09\n",
      "['']\n",
      "epoch: 157, d_loss: 0.45,         d_acc: 0.80, g_loss: 1.10\n",
      "['']\n",
      "epoch: 158, d_loss: 0.43,         d_acc: 0.83, g_loss: 1.10\n",
      "['']\n",
      "epoch: 159, d_loss: 0.53,         d_acc: 0.70, g_loss: 1.10\n",
      "['']\n",
      "epoch: 160, d_loss: 0.45,         d_acc: 0.79, g_loss: 1.10\n",
      "['']\n",
      "epoch: 161, d_loss: 0.47,         d_acc: 0.79, g_loss: 1.10\n",
      "['']\n",
      "epoch: 162, d_loss: 0.51,         d_acc: 0.75, g_loss: 1.10\n",
      "['']\n",
      "epoch: 163, d_loss: 0.51,         d_acc: 0.75, g_loss: 1.10\n",
      "['']\n",
      "epoch: 164, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.10\n",
      "['']\n",
      "epoch: 165, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.10\n",
      "['']\n",
      "epoch: 166, d_loss: 0.42,         d_acc: 0.83, g_loss: 1.11\n",
      "['']\n",
      "epoch: 167, d_loss: 0.53,         d_acc: 0.72, g_loss: 1.11\n",
      "['']\n",
      "epoch: 168, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.11\n",
      "['']\n",
      "epoch: 169, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.11\n",
      "['']\n",
      "epoch: 170, d_loss: 0.54,         d_acc: 0.70, g_loss: 1.11\n",
      "['']\n",
      "epoch: 171, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.11\n",
      "['']\n",
      "epoch: 172, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.11\n",
      "['']\n",
      "epoch: 173, d_loss: 0.44,         d_acc: 0.81, g_loss: 1.11\n",
      "['']\n",
      "epoch: 174, d_loss: 0.46,         d_acc: 0.80, g_loss: 1.11\n",
      "['']\n",
      "epoch: 175, d_loss: 0.50,         d_acc: 0.77, g_loss: 1.11\n",
      "['']\n",
      "epoch: 176, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.11\n",
      "['']\n",
      "epoch: 177, d_loss: 0.42,         d_acc: 0.83, g_loss: 1.11\n",
      "['']\n",
      "epoch: 178, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.11\n",
      "['']\n",
      "epoch: 179, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.11\n",
      "['']\n",
      "epoch: 180, d_loss: 0.43,         d_acc: 0.81, g_loss: 1.12\n",
      "['']\n",
      "epoch: 181, d_loss: 0.49,         d_acc: 0.78, g_loss: 1.12\n",
      "['']\n",
      "epoch: 182, d_loss: 0.53,         d_acc: 0.72, g_loss: 1.12\n",
      "['']\n",
      "epoch: 183, d_loss: 0.45,         d_acc: 0.82, g_loss: 1.12\n",
      "['']\n",
      "epoch: 184, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.12\n",
      "['']\n",
      "epoch: 185, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.12\n",
      "['']\n",
      "epoch: 186, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.12\n",
      "['']\n",
      "epoch: 187, d_loss: 0.49,         d_acc: 0.76, g_loss: 1.12\n",
      "['']\n",
      "epoch: 188, d_loss: 0.51,         d_acc: 0.74, g_loss: 1.12\n",
      "['']\n",
      "epoch: 189, d_loss: 0.43,         d_acc: 0.83, g_loss: 1.12\n",
      "['']\n",
      "epoch: 190, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.12\n",
      "['']\n",
      "epoch: 191, d_loss: 0.48,         d_acc: 0.76, g_loss: 1.12\n",
      "['']\n",
      "epoch: 192, d_loss: 0.49,         d_acc: 0.77, g_loss: 1.13\n",
      "['']\n",
      "epoch: 193, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.12\n",
      "['']\n",
      "epoch: 194, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.13\n",
      "['']\n",
      "epoch: 195, d_loss: 0.43,         d_acc: 0.81, g_loss: 1.13\n",
      "['']\n",
      "epoch: 196, d_loss: 0.45,         d_acc: 0.81, g_loss: 1.13\n",
      "['']\n",
      "epoch: 197, d_loss: 0.43,         d_acc: 0.80, g_loss: 1.13\n",
      "['']\n",
      "epoch: 198, d_loss: 0.45,         d_acc: 0.81, g_loss: 1.13\n",
      "['']\n",
      "epoch: 199, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.13\n",
      "['']\n",
      "epoch: 200, d_loss: 0.46,         d_acc: 0.76, g_loss: 1.14\n",
      "['']\n",
      "epoch: 201, d_loss: 0.38,         d_acc: 0.86, g_loss: 1.14\n",
      "['']\n",
      "epoch: 202, d_loss: 0.46,         d_acc: 0.79, g_loss: 1.14\n",
      "['']\n",
      "epoch: 203, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.15\n",
      "['']\n",
      "epoch: 204, d_loss: 0.41,         d_acc: 0.84, g_loss: 1.15\n",
      "['']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 205, d_loss: 0.46,         d_acc: 0.81, g_loss: 1.15\n",
      "['']\n",
      "epoch: 206, d_loss: 0.45,         d_acc: 0.80, g_loss: 1.16\n",
      "['']\n",
      "epoch: 207, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.16\n",
      "['']\n",
      "epoch: 208, d_loss: 0.44,         d_acc: 0.81, g_loss: 1.16\n",
      "['']\n",
      "epoch: 209, d_loss: 0.41,         d_acc: 0.82, g_loss: 1.17\n",
      "['']\n",
      "epoch: 210, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.17\n",
      "['']\n",
      "epoch: 211, d_loss: 0.50,         d_acc: 0.73, g_loss: 1.17\n",
      "['']\n",
      "epoch: 212, d_loss: 0.40,         d_acc: 0.86, g_loss: 1.17\n",
      "['']\n",
      "epoch: 213, d_loss: 0.44,         d_acc: 0.79, g_loss: 1.17\n",
      "['']\n",
      "epoch: 214, d_loss: 0.43,         d_acc: 0.80, g_loss: 1.18\n",
      "['']\n",
      "epoch: 215, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.17\n",
      "['']\n",
      "epoch: 216, d_loss: 0.45,         d_acc: 0.80, g_loss: 1.18\n",
      "['']\n",
      "epoch: 217, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.17\n",
      "['']\n",
      "epoch: 218, d_loss: 0.51,         d_acc: 0.74, g_loss: 1.17\n",
      "['']\n",
      "epoch: 219, d_loss: 0.47,         d_acc: 0.81, g_loss: 1.17\n",
      "['']\n",
      "epoch: 220, d_loss: 0.44,         d_acc: 0.81, g_loss: 1.17\n",
      "['']\n",
      "epoch: 221, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.17\n",
      "['']\n",
      "epoch: 222, d_loss: 0.50,         d_acc: 0.73, g_loss: 1.16\n",
      "['']\n",
      "epoch: 223, d_loss: 0.47,         d_acc: 0.77, g_loss: 1.16\n",
      "['']\n",
      "epoch: 224, d_loss: 0.44,         d_acc: 0.81, g_loss: 1.16\n",
      "['']\n",
      "epoch: 225, d_loss: 0.45,         d_acc: 0.80, g_loss: 1.16\n",
      "['']\n",
      "epoch: 226, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.16\n",
      "['']\n",
      "epoch: 227, d_loss: 0.41,         d_acc: 0.84, g_loss: 1.16\n",
      "['']\n",
      "epoch: 228, d_loss: 0.37,         d_acc: 0.86, g_loss: 1.16\n",
      "['']\n",
      "epoch: 229, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.16\n",
      "['']\n",
      "epoch: 230, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.16\n",
      "['']\n",
      "epoch: 231, d_loss: 0.44,         d_acc: 0.81, g_loss: 1.16\n",
      "['']\n",
      "epoch: 232, d_loss: 0.39,         d_acc: 0.85, g_loss: 1.17\n",
      "['']\n",
      "epoch: 233, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.17\n",
      "['']\n",
      "epoch: 234, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.17\n",
      "['']\n",
      "epoch: 235, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.17\n",
      "['']\n",
      "epoch: 236, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.17\n",
      "['']\n",
      "epoch: 237, d_loss: 0.49,         d_acc: 0.75, g_loss: 1.17\n",
      "['']\n",
      "epoch: 238, d_loss: 0.43,         d_acc: 0.80, g_loss: 1.17\n",
      "['']\n",
      "epoch: 239, d_loss: 0.45,         d_acc: 0.79, g_loss: 1.17\n",
      "['']\n",
      "epoch: 240, d_loss: 0.41,         d_acc: 0.83, g_loss: 1.17\n",
      "['']\n",
      "epoch: 241, d_loss: 0.39,         d_acc: 0.85, g_loss: 1.18\n",
      "['']\n",
      "epoch: 242, d_loss: 0.47,         d_acc: 0.77, g_loss: 1.18\n",
      "['']\n",
      "epoch: 243, d_loss: 0.45,         d_acc: 0.81, g_loss: 1.18\n",
      "['']\n",
      "epoch: 244, d_loss: 0.46,         d_acc: 0.76, g_loss: 1.18\n",
      "['']\n",
      "epoch: 245, d_loss: 0.43,         d_acc: 0.81, g_loss: 1.18\n",
      "['']\n",
      "epoch: 246, d_loss: 0.43,         d_acc: 0.83, g_loss: 1.18\n",
      "['']\n",
      "epoch: 247, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.18\n",
      "['']\n",
      "epoch: 248, d_loss: 0.39,         d_acc: 0.84, g_loss: 1.19\n",
      "['']\n",
      "epoch: 249, d_loss: 0.39,         d_acc: 0.85, g_loss: 1.19\n",
      "['']\n",
      "epoch: 250, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.19\n",
      "['']\n",
      "epoch: 251, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.19\n",
      "['']\n",
      "epoch: 252, d_loss: 0.50,         d_acc: 0.73, g_loss: 1.19\n",
      "['']\n",
      "epoch: 253, d_loss: 0.40,         d_acc: 0.84, g_loss: 1.20\n",
      "['']\n",
      "epoch: 254, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.20\n",
      "['']\n",
      "epoch: 255, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.20\n",
      "['']\n",
      "epoch: 256, d_loss: 0.43,         d_acc: 0.81, g_loss: 1.20\n",
      "['']\n",
      "epoch: 257, d_loss: 0.37,         d_acc: 0.87, g_loss: 1.20\n",
      "['']\n",
      "epoch: 258, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.20\n",
      "['']\n",
      "epoch: 259, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.20\n",
      "['']\n",
      "epoch: 260, d_loss: 0.45,         d_acc: 0.78, g_loss: 1.20\n",
      "['']\n",
      "epoch: 261, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.20\n",
      "['']\n",
      "epoch: 262, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.20\n",
      "['']\n",
      "epoch: 263, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.21\n",
      "['']\n",
      "epoch: 264, d_loss: 0.44,         d_acc: 0.78, g_loss: 1.21\n",
      "['']\n",
      "epoch: 265, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.21\n",
      "['']\n",
      "epoch: 266, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.20\n",
      "['']\n",
      "epoch: 267, d_loss: 0.46,         d_acc: 0.80, g_loss: 1.20\n",
      "['']\n",
      "epoch: 268, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.19\n",
      "['']\n",
      "epoch: 269, d_loss: 0.41,         d_acc: 0.82, g_loss: 1.19\n",
      "[' to']\n",
      "epoch: 270, d_loss: 0.46,         d_acc: 0.79, g_loss: 1.18\n",
      "['']\n",
      "epoch: 271, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.15\n",
      "['']\n",
      "epoch: 272, d_loss: 0.47,         d_acc: 0.79, g_loss: 1.12\n",
      "[' to to to to to to']\n",
      "epoch: 273, d_loss: 0.44,         d_acc: 0.84, g_loss: 1.08\n",
      "[' to to']\n",
      "epoch: 274, d_loss: 0.48,         d_acc: 0.82, g_loss: 1.04\n",
      "[' to to to to to']\n",
      "epoch: 275, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.94\n",
      "[' to to to to to to to to to to']\n",
      "epoch: 276, d_loss: 0.62,         d_acc: 0.63, g_loss: 0.85\n",
      "[' to to to to to to to to to to']\n",
      "epoch: 277, d_loss: 0.59,         d_acc: 0.57, g_loss: 0.74\n",
      "[' to to to to to to to to to to']\n",
      "epoch: 278, d_loss: 0.71,         d_acc: 0.49, g_loss: 0.69\n",
      "[' to to to to to the the the the the the the the the the to to']\n",
      "epoch: 279, d_loss: 0.72,         d_acc: 0.32, g_loss: 0.56\n",
      "[' to to to to to to to to to to']\n",
      "epoch: 280, d_loss: 0.92,         d_acc: 0.28, g_loss: 0.42\n",
      "[' to to to to to to to to to to to to the the the the the the the the and the to to to to']\n",
      "epoch: 281, d_loss: 0.96,         d_acc: 0.34, g_loss: 0.37\n",
      "[' to to to to to to to to to to to to the the the the the the the the and the to to to to']\n",
      "epoch: 282, d_loss: 1.33,         d_acc: 0.29, g_loss: 0.24\n",
      "[' to to to to to to to to to to to to to to the the the the the the the the and the to to to to']\n",
      "epoch: 283, d_loss: 5.99,         d_acc: 0.33, g_loss: 0.17\n",
      "[' to to to to to to to to to to to to the the the the the the the the the the to to to to']\n",
      "epoch: 284, d_loss: 3.01,         d_acc: 0.31, g_loss: 0.10\n",
      "[' to to to to to to to to to to to to to to to to to to to the the the and and and and and and and and and the to to to']\n",
      "epoch: 285, d_loss: 2.72,         d_acc: 0.31, g_loss: 0.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to the the the the the and and and and and and and and a and to the to to to to']\n",
      "epoch: 286, d_loss: 10.63,         d_acc: 0.27, g_loss: 0.04\n",
      "[' to to to to to to to to to to to to to to to to to to to the the to the the the the the the the the and and a a a a a a a a to the the to to to to']\n",
      "epoch: 287, d_loss: 17.76,         d_acc: 0.30, g_loss: 1.31\n",
      "[' to to to to to to to to to to to to to to to to to the to to the the the the the and and and and a and and a a and to the the to to to to']\n",
      "epoch: 288, d_loss: 10.64,         d_acc: 0.32, g_loss: 0.79\n",
      "[' to to to to to to to to to to to to to to to the to the the the the the the the the the the the and and a a a a a a a a to the the to to to to']\n",
      "epoch: 289, d_loss: 4.17,         d_acc: 0.26, g_loss: 0.12\n",
      "[' to to to to to to to to to to to to to to to to the to to the the the the the and and and and and and and and a and to the the to to to to']\n",
      "epoch: 290, d_loss: 3.58,         d_acc: 0.29, g_loss: 0.19\n",
      "[' to to to to to to to to to to to to to to to the to the the the the the the the the the the the and and a a a a a a a a to the the the the to to']\n",
      "epoch: 291, d_loss: 2.12,         d_acc: 0.25, g_loss: 0.52\n",
      "[' to to to to to to to to to to to the to the the the the the the the and and and and the and and and and a and in in in in of in in of of in the and and the the the to']\n",
      "epoch: 292, d_loss: 2.06,         d_acc: 0.37, g_loss: 0.62\n",
      "[' to to to to to to to to to to to the to the the the the the the the and and and and the and and and and a and in in in in of in in in of in the and and the the the to']\n",
      "epoch: 293, d_loss: 1.71,         d_acc: 0.50, g_loss: 0.68\n",
      "[' the to the the and the and and the to the the a and and a a in a in a of of in of in in of of of you our this this it it for it it for for this and our in in in and the']\n",
      "epoch: 294, d_loss: 1.83,         d_acc: 0.39, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to the the the the the the the the the the the the the and and a a a a a a a a in a to and the the the to to']\n",
      "epoch: 295, d_loss: 2.06,         d_acc: 0.45, g_loss: 1.03\n",
      "[' to to to to to to to to to to the to the the the the the the the the and the and the the and and and and and a a in in in in in in in a the and the the the to to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 296, d_loss: 2.78,         d_acc: 0.53, g_loss: 1.15\n",
      "[' to to to to to to to to to to the to to the the the the the the the the the and the the and and and and and a a a a in a in in in a the and the the the to to']\n",
      "epoch: 297, d_loss: 2.95,         d_acc: 0.45, g_loss: 2.53\n",
      "[' to to to to the to the the the to to the and the the and and and and and and and a and a and and a a a in a of of of of our of of our our of the a and and and the to']\n",
      "epoch: 298, d_loss: 3.13,         d_acc: 0.47, g_loss: 2.62\n",
      "[' to to to to the to the the to to the and the the and and and and and and and and and a and and a a a a a of of of of of of of of our of the a and and and the to']\n",
      "epoch: 299, d_loss: 3.03,         d_acc: 0.51, g_loss: 3.74\n",
      "[' to to to the the the the the the to to the and the the and and and and a and a a a a a a a a a in in of of our our our our our our our of and in and and and the to']\n",
      "epoch: 300, d_loss: 3.90,         d_acc: 0.34, g_loss: 2.09\n",
      "[' to to to the the the the the the to to the and the the and and and and a and a a a a a a a a a in in of of our our our our our our our of and a and and and the to']\n",
      "epoch: 301, d_loss: 3.58,         d_acc: 0.34, g_loss: 4.99\n",
      "[' did inspired vaccine has and tribute dark it made 5816 letâ€™s under protect aligns create senate holidays have reminded essential continue 9687 principles ways hear until']\n",
      "epoch: 302, d_loss: 2.47,         d_acc: 0.39, g_loss: 2.37\n",
      "[' to to to the the the the the the to the the and and and and and a a a and a a a in a a in in in of in our our our our you our our you you our and in and and and the to']\n",
      "epoch: 303, d_loss: 1.42,         d_acc: 0.54, g_loss: 4.25\n",
      "[\" change today from make all the time his all on in i we world weâ€™re but as day climate me love and climate now not now how early life how like voter save it's now voter white americans now everyone these across better out could all democracy early out\"]\n",
      "epoch: 304, d_loss: 3.41,         d_acc: 0.60, g_loss: 7.71\n",
      "[' in and a a a in of a a to and a this our our our you it it it of for is for is it for is it for that vote i that we we make that i we we all our is our you you a to']\n",
      "epoch: 305, d_loss: 1.72,         d_acc: 0.64, g_loss: 9.49\n",
      "[' the the the the and and and the the to the and in a a in in of of of in of of of our of of our our our you you it this it it for this it for for this a our a in in the to']\n",
      "epoch: 306, d_loss: 7.77,         d_acc: 0.55, g_loss: 12.90\n",
      "[' the the the the and and the the the the and a a a in in in in of a of of of of of of of of our you our this this it it it this it it it you a our a a a the to']\n",
      "epoch: 307, d_loss: 6.84,         d_acc: 0.57, g_loss: 15.57\n",
      "[' to the to to the the the the the to the and and and a a a a a a in in in in in in in in in of of our our you you you our you you you our and in and and and to']\n",
      "epoch: 308, d_loss: 1.82,         d_acc: 0.55, g_loss: 12.78\n",
      "[' as the we is is in to on as the for the our you one with that all my us have one in but have one us with election are can at work out before but because hope people have work his by will make or our if if that']\n",
      "epoch: 309, d_loss: 0.97,         d_acc: 0.68, g_loss: 12.47\n",
      "[\" next keep early more by make across itâ€™s can more a from at power down democracy how climate presidency let's no mail no many even things left under things voter yours preexisting changing center canâ€™t virus yours during polling administration seen hotline care difference this than americans save\"]\n",
      "epoch: 310, d_loss: 1.11,         d_acc: 0.62, g_loss: 10.06\n",
      "[' to the to the the and the the the to the and a a a in in in in a in of of of in of of of of our our this you this this this you this this this you and of and and and to']\n",
      "epoch: 311, d_loss: 1.92,         d_acc: 0.60, g_loss: 8.47\n",
      "[' to the to to the the the the the to the and and and a a a in in a in in in of in in in in of of of you you you you this our you you this our and of and and and to']\n",
      "epoch: 312, d_loss: 1.33,         d_acc: 0.62, g_loss: 6.32\n",
      "[' to to to to the the to to the to to the and and and and and a a and a a a in a a in in in in in our of our our our of our our our of the in the the the to']\n",
      "epoch: 313, d_loss: 0.71,         d_acc: 0.69, g_loss: 4.26\n",
      "[' to the to to and and the the the to the and a a a in in in in a in of of of in of of of our our our this this this this it you this this it you and our and and and to']\n",
      "epoch: 314, d_loss: 0.70,         d_acc: 0.62, g_loss: 3.55\n",
      "[' to the to to the and the the the to the and a a a in in in in a in of of of in of of of our our our this you this this it you this this it you and of and and and to']\n",
      "epoch: 315, d_loss: 0.50,         d_acc: 0.71, g_loss: 3.30\n",
      "[' the and the the a in and and and to to and in of of of our our this this of you this it it this this it this it for for all vote all that that is all that that is in it a in in to']\n",
      "epoch: 316, d_loss: 0.63,         d_acc: 0.62, g_loss: 2.48\n",
      "[' to the to to the and to the the to the and and a a in in in in a in of of of in of of of our our our this you this this this our this this this you and of and and and to']\n",
      "epoch: 317, d_loss: 0.54,         d_acc: 0.68, g_loss: 2.12\n",
      "[' to the to to the and to to the to to the and a a a a in in a in in of of in of of of of our our you you this this this our you this this our and of the and and']\n",
      "epoch: 318, d_loss: 0.55,         d_acc: 0.67, g_loss: 2.59\n",
      "[' to to to the the to to the to the and and and a a a a and a a in in a in in in in of of our our our our our of our our our of the in the the the']\n",
      "epoch: 319, d_loss: 0.53,         d_acc: 0.68, g_loss: 2.11\n",
      "[' to to to the the to to to to the the and and and and a a and a a in in a a in a in in in of of our our our in of our our of the in the the the']\n",
      "epoch: 320, d_loss: 0.54,         d_acc: 0.66, g_loss: 2.98\n",
      "[' to to to to the to to to to to the and and and and a a and and a a a and a a a a in in of in of of of in of of of in the a to to to']\n",
      "epoch: 321, d_loss: 0.53,         d_acc: 0.69, g_loss: 6.40\n",
      "[' the to to the the to to the to the and a a a a in in a a in of of in in of of of our our you you you you you of you you you our the of the the the']\n",
      "epoch: 322, d_loss: 0.44,         d_acc: 0.70, g_loss: 1.85\n",
      "[' to to to the the to to to to the the and and a and a a and a a in in a a in a in in in of of of our our in of of our of the in to the the']\n",
      "epoch: 323, d_loss: 0.44,         d_acc: 0.71, g_loss: 1.87\n",
      "[' to to to to the to to to to the the and and and and a a and and a a in a a a a in in in of of of of of in of of of in the a to to to']\n",
      "epoch: 324, d_loss: 0.45,         d_acc: 0.73, g_loss: 1.95\n",
      "[' to to to the the to to to to the the and and a and a a and a a in in a in in in in in of of of our our our in of our our of the in to the to']\n",
      "epoch: 325, d_loss: 0.43,         d_acc: 0.67, g_loss: 2.06\n",
      "[' to to to to the to to to to the and and and and a a and and a a in a a a a in in in of of of of of a of of of in to a to to to']\n",
      "epoch: 326, d_loss: 0.42,         d_acc: 0.69, g_loss: 2.28\n",
      "[' to to to the the to to the to the and a a a a in in a a in of of in in of in of of of our our you you you of our our you of the in to the the']\n",
      "epoch: 327, d_loss: 0.39,         d_acc: 0.73, g_loss: 2.13\n",
      "[' to to to to to to to to to the and and and and a a and and a a a a a a a a in in in in of of of a in in of in to a to to to']\n",
      "epoch: 328, d_loss: 0.35,         d_acc: 0.75, g_loss: 2.24\n",
      "[' to to to the the to to the to the and a a in a of in a in in of of in of of of of our our you you you you you of you you you our the of to the the']\n",
      "epoch: 329, d_loss: 0.38,         d_acc: 0.72, g_loss: 2.13\n",
      "[' to to to to the to to to the the and and a and a a and a a in in a a in a in in in of of of of of in of of of in the a to to to']\n",
      "epoch: 330, d_loss: 0.41,         d_acc: 0.71, g_loss: 2.11\n",
      "[' to to to the the to to the to the and a a in a in in a in in of of in of of in of our our you our you you you of our our you of the in to to to']\n",
      "epoch: 331, d_loss: 0.39,         d_acc: 0.72, g_loss: 2.11\n",
      "[' to to to to to to to to the and and a and a a and and a a in a a a a in in in of in of of of a in of of in to a to to to']\n",
      "epoch: 332, d_loss: 0.34,         d_acc: 0.77, g_loss: 2.03\n",
      "[' to to to to the to to to the and a and a a in in a a in in in in in in in in of of our of our our our in of of our in the in to to to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 333, d_loss: 0.36,         d_acc: 0.77, g_loss: 2.08\n",
      "[' the the the and a to the and to to a in our of you our this this our you this it it this this it this it for for vote is vote vote vote this is is is it and you the the the']\n",
      "epoch: 334, d_loss: 0.42,         d_acc: 0.68, g_loss: 2.05\n",
      "[' to to to the the to to to to the and a a in a in in a a in in of in in of in of of of our our our our our in our our our of the in to to to']\n",
      "epoch: 335, d_loss: 0.43,         d_acc: 0.68, g_loss: 1.91\n",
      "[' to to to to the to to to the and a and in a in in a a in in of in in in in of of of our our our our our in of our our in the in to to to']\n",
      "epoch: 336, d_loss: 0.38,         d_acc: 0.72, g_loss: 1.83\n",
      "[' to to to the the to to the to the and in a in a of in a in in of of in of of of of our our our our you you you of our our our of the in to to to']\n",
      "epoch: 337, d_loss: 0.40,         d_acc: 0.71, g_loss: 1.90\n",
      "[' to to to to to to to the the a and a and in a and a a in in a in in a in in in of of of of of in of of of in to a to to to']\n",
      "epoch: 338, d_loss: 0.39,         d_acc: 0.75, g_loss: 1.81\n",
      "[' to to to to the to to to the and a and a a in in a a a in in a in in in in of of of of our our of in of of of in to a to to to']\n",
      "epoch: 339, d_loss: 0.43,         d_acc: 0.71, g_loss: 1.73\n",
      "[' to to to to the to to the and a and a a in in a a a in in a in in in in of of of of of of of in of of of in to a to to to']\n",
      "epoch: 340, d_loss: 0.37,         d_acc: 0.77, g_loss: 1.69\n",
      "[' the the the and and to the the to to and in our of you our this you our you you this it you this this this it it it is for is is is you for for for this and our the to to']\n",
      "epoch: 341, d_loss: 0.44,         d_acc: 0.70, g_loss: 1.64\n",
      "[' to to to the the to to to the and a a in a of in a in in of of in in of in of of of our our our our our in our our our in the a to to to']\n",
      "epoch: 342, d_loss: 0.41,         d_acc: 0.69, g_loss: 1.56\n",
      "[' to to to to the to to to the and a a in a in in a a in in of in in in in of of of our of our our our in of of of in to a to']\n",
      "epoch: 343, d_loss: 0.46,         d_acc: 0.73, g_loss: 1.51\n",
      "[' to to to to the to to the and a and in a in in a a in in of in in in in in of of of of our our of in of of of in to a to']\n",
      "epoch: 344, d_loss: 0.33,         d_acc: 0.84, g_loss: 1.49\n",
      "[' the the the and and to the the to to a in you of you our this this our you this it it this this it this it for for is for is is is this for for for this and our to to to']\n",
      "epoch: 345, d_loss: 0.37,         d_acc: 0.79, g_loss: 1.58\n",
      "[' to to to to the to to to the and in a in a of in a in in of of in in of in of of of our our our our our in of of of in to a to']\n",
      "epoch: 346, d_loss: 0.40,         d_acc: 0.78, g_loss: 1.57\n",
      "[' to to to the the to to the to the and in a of in of of in in of of our of of our of our our our you you you you you of our our our of the in to']\n",
      "epoch: 347, d_loss: 0.36,         d_acc: 0.82, g_loss: 1.58\n",
      "[' to to to the the to to to to the and in a of in of of in in of of our of of of of our our our you our you you you of our our our of the in to']\n",
      "epoch: 348, d_loss: 0.33,         d_acc: 0.78, g_loss: 1.63\n",
      "[' to a and and in in the and a to the the of you vote it all is your all for vote all your i all that your that your i i make we on make make all i i i all in for the the the']\n",
      "epoch: 349, d_loss: 0.33,         d_acc: 0.82, g_loss: 1.68\n",
      "[' to to to the the to to the to the and in a of in our of in of of our our of of our of our our our you you you you you of our our our of the in to']\n",
      "epoch: 350, d_loss: 0.36,         d_acc: 0.80, g_loss: 1.79\n",
      "[' the to to the and to to the to to and a our of you of you you of our you this this you you this you this this this it it it it it our this this this our the of to to']\n",
      "epoch: 351, d_loss: 0.34,         d_acc: 0.79, g_loss: 1.89\n",
      "[' to the the the and a to the and to to a of it you for this is for you it it is is it for is for is vote vote all vote all all all it vote is is it and you to to to']\n",
      "epoch: 352, d_loss: 0.34,         d_acc: 0.77, g_loss: 1.77\n",
      "[' to to to the the to to to to the and in a of in of of in in of of our of of of of our our our our our you our our in our our our in to a to']\n",
      "epoch: 353, d_loss: 0.34,         d_acc: 0.79, g_loss: 1.90\n",
      "[' the to the the and to to the to to and in our of you our this you our our you this this you you this you this it it for it for it it you this this this you the of to']\n",
      "epoch: 354, d_loss: 0.35,         d_acc: 0.83, g_loss: 1.87\n",
      "[' to to to the the to to to to the and in a of in of of in in of of our of of of of our our our our our you our our in our our our in to a']\n",
      "epoch: 355, d_loss: 0.34,         d_acc: 0.86, g_loss: 1.95\n",
      "[' to to to to the to to to the and in a in a of in a in in of of in of of in of of of our our our our our in of of of in to a']\n",
      "epoch: 356, d_loss: 0.33,         d_acc: 0.89, g_loss: 1.98\n",
      "[' to to to the the to to to to the and in a of in of of in in of of our of of of of our our our our our you our our in our of our in to a']\n",
      "epoch: 357, d_loss: 0.34,         d_acc: 0.96, g_loss: 1.79\n",
      "[' to to to the the to to to to the and in a of in of of in in of of our of of of of our our our our our you our our in our of of in to a']\n",
      "epoch: 358, d_loss: 0.36,         d_acc: 0.91, g_loss: 1.88\n",
      "[' the to to the the to to the to to and a our of you of this you of our our this this our you you you this this this it it it it this our this you this our the in to']\n",
      "epoch: 359, d_loss: 0.33,         d_acc: 0.96, g_loss: 1.83\n",
      "[' to to to the the to to to to the and in a of in of of in in in of of in of of of of our our our our our our our in of of of in to a']\n",
      "epoch: 360, d_loss: 0.28,         d_acc: 0.97, g_loss: 1.87\n",
      "[' to to to to the to to to to the and in a of in of of a in in of of in of of in of of of our our our our our in of of of in to and']\n",
      "epoch: 361, d_loss: 0.30,         d_acc: 0.96, g_loss: 1.99\n",
      "[' to to to the the to to to to the and in a of in our of in in of our our of of our of our our our you our you our our in our of of in to a']\n",
      "epoch: 362, d_loss: 0.32,         d_acc: 0.96, g_loss: 1.88\n",
      "[' to to to to the to to to to the and in a of in of of in in in of of in of of in of of of our our our our our in of of of in to and']\n",
      "epoch: 363, d_loss: 0.29,         d_acc: 0.95, g_loss: 1.91\n",
      "[' to to to the the to to the to to and a of in our in our our in of of our our of our our of our you our you you you you you of our our our in to a']\n",
      "epoch: 364, d_loss: 0.31,         d_acc: 0.96, g_loss: 1.86\n",
      "[' the the the and and to to the to to a in you of this our it this our you you it it you this it this it it it for for for for it you it this this our the in to']\n",
      "epoch: 365, d_loss: 0.31,         d_acc: 0.93, g_loss: 1.91\n",
      "[' to to to the the to to to to and a of a of in our of in of of our our of our our of our our our you you you you our of our of our in to a']\n",
      "epoch: 366, d_loss: 0.30,         d_acc: 0.96, g_loss: 1.89\n",
      "[' to to to to the to to to the and in a in a of in a in in of of in in of in of of of our of our of of in of in of a to and']\n",
      "epoch: 367, d_loss: 0.29,         d_acc: 1.00, g_loss: 1.97\n",
      "[' to to to the the to to to to and a of a of in our of in of of our our of of our of our our our you our you you our in our of of in to and']\n",
      "epoch: 368, d_loss: 0.25,         d_acc: 0.97, g_loss: 1.98\n",
      "[' to to to the the to to the to to and a our in you of you you of our our you this our you you our you this this this this this this this our you you you of to a']\n",
      "epoch: 369, d_loss: 0.26,         d_acc: 0.97, g_loss: 2.08\n",
      "[' to to to to the to to to the and in a of in of of a in in of of in of of in of of of our our our our of in of in of a to and']\n",
      "epoch: 370, d_loss: 0.28,         d_acc: 0.97, g_loss: 2.01\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of our of of in of in in a to and']\n",
      "epoch: 371, d_loss: 0.28,         d_acc: 0.98, g_loss: 2.10\n",
      "[' to to to to to to to to the and in a of a of in a in in of of in of of in of of of of of our of of in of in in a to and']\n",
      "epoch: 372, d_loss: 0.26,         d_acc: 0.98, g_loss: 2.10\n",
      "[' to to to the the to to to to the and of a of in our of in in of of our of of of of our our our our our our our our in of of of in to and']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 373, d_loss: 0.25,         d_acc: 0.98, g_loss: 2.07\n",
      "[' to to to the the to to the to to and a of in our of you our in of our our you our our our our our you you you you you you you of our our our in to and']\n",
      "epoch: 374, d_loss: 0.23,         d_acc: 0.99, g_loss: 2.07\n",
      "[' to to to the the to to the to to and a of in our of you our in of our our you our our our our our you you you you you you you of our our our in to and']\n",
      "epoch: 375, d_loss: 0.24,         d_acc: 0.99, g_loss: 2.11\n",
      "[' to to to the the to to to to and a of a of in our of in of of our our of of our of our our our our our our our our in of of of in to and']\n",
      "epoch: 376, d_loss: 0.25,         d_acc: 1.00, g_loss: 2.15\n",
      "[' to to to the the to to the to to and a our in you of you you of our our you this our you you our you this you this this this this this of you our our of to and']\n",
      "epoch: 377, d_loss: 0.23,         d_acc: 0.99, g_loss: 2.11\n",
      "[' to to to to the to to to to the and in a of in of of in in of of our of of of of of our our our our our our our in of in in a to and']\n",
      "epoch: 378, d_loss: 0.25,         d_acc: 0.98, g_loss: 2.05\n",
      "[' to to to to to to to to the and in a of in of in a in in of of in of of in of of of of of of of of a in in in a the']\n",
      "epoch: 379, d_loss: 0.23,         d_acc: 1.00, g_loss: 2.17\n",
      "[' to to to to the to to to the and in a of in of of a in in of of in of of in of of of our of our of of a in in in a the']\n",
      "epoch: 380, d_loss: 0.25,         d_acc: 0.99, g_loss: 2.20\n",
      "[' to to to to to to to to the and in a of in of of a in in of of in of of in of of of of of of of of a in in in a the']\n",
      "epoch: 381, d_loss: 0.24,         d_acc: 0.98, g_loss: 2.08\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of of of of a in in in a the']\n",
      "epoch: 382, d_loss: 0.22,         d_acc: 0.99, g_loss: 2.22\n",
      "[' to to to the the to to the to to and a our in our of you our of our our you you our our you our you you you you you you you you of our of of in to and']\n",
      "epoch: 383, d_loss: 0.24,         d_acc: 0.97, g_loss: 2.06\n",
      "[' to to to the the to to to to and a of a of in our of in of of our our of of our of our our our our our our our our in of in of a the']\n",
      "epoch: 384, d_loss: 0.24,         d_acc: 0.99, g_loss: 2.17\n",
      "[' to to to to the to to to to the and in a of in our of in in of of our of of of of of our our our our our our of in of in in a the']\n",
      "epoch: 385, d_loss: 0.22,         d_acc: 0.99, g_loss: 2.08\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of of of of a in a in and the']\n",
      "epoch: 386, d_loss: 0.22,         d_acc: 0.99, g_loss: 2.27\n",
      "[' to to to to the to to to to the and of a of in our of in in of of our of of of of of our our our our our our of in of in in a the']\n",
      "epoch: 387, d_loss: 0.23,         d_acc: 0.99, g_loss: 2.13\n",
      "[' to to to to to to to to the and in a of in of of a in in of of in of of in of of of of of of of of a in in in a the']\n",
      "epoch: 388, d_loss: 0.24,         d_acc: 0.98, g_loss: 2.23\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of of of of a in a a and the']\n",
      "epoch: 389, d_loss: 0.22,         d_acc: 1.00, g_loss: 2.16\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of of of of a in a a and the']\n",
      "epoch: 390, d_loss: 0.22,         d_acc: 0.98, g_loss: 2.10\n",
      "[' to to to to the to to to the and in a of in of of in in in of of in of of in of of of our of our of of a in in in a the']\n",
      "epoch: 391, d_loss: 0.21,         d_acc: 0.99, g_loss: 2.02\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of of of of a in a a and to']\n",
      "epoch: 392, d_loss: 0.24,         d_acc: 0.99, g_loss: 2.20\n",
      "[' the the the and a to the and to to in our for this is it vote is this for for vote vote for is vote for vote vote vote all vote all vote vote this for it it our to a']\n",
      "epoch: 393, d_loss: 0.22,         d_acc: 0.99, g_loss: 2.15\n",
      "[' to to to to to to to to the and in a of in of in a in in of of in in of in of of of of of of of of a in a a and to']\n",
      "epoch: 394, d_loss: 0.23,         d_acc: 0.99, g_loss: 2.30\n",
      "[' to to to the the to to the to to and in you of you our this you our you you this this you you this you this this this this this this this this of our our our in to the']\n",
      "epoch: 395, d_loss: 0.23,         d_acc: 0.98, g_loss: 2.09\n",
      "[' to to to to to to to to the and in a in a of in a in in of of in in of in of of of of of of of of a in a a and to']\n",
      "epoch: 396, d_loss: 0.24,         d_acc: 0.98, g_loss: 2.35\n",
      "[' to to to to to to to the and in a in a of in a in in in of in in in in in of of of of of in in a a a a and to']\n",
      "epoch: 397, d_loss: 0.24,         d_acc: 0.98, g_loss: 1.87\n",
      "[' to to to to the to to to to the and of a of in our of in of of of our of of of of of our our our our our of of a in in in and to']\n",
      "epoch: 398, d_loss: 0.25,         d_acc: 0.98, g_loss: 2.08\n",
      "[' the the the and and to the the to to a of it you for this is for you it it for is it for for it for is for is for is for for our this you you of to the']\n",
      "epoch: 399, d_loss: 0.25,         d_acc: 0.98, g_loss: 1.90\n",
      "[' to to to the the to to to to and a of in our in our of in of of our our of of our of our our our our our our our our a in in in a to']\n",
      "epoch: 400, d_loss: 0.24,         d_acc: 0.98, g_loss: 1.96\n",
      "[' the to to the the to to the to to a in you of this our this this our you you this it you this this you this this this it this it this this of our of our in the']\n",
      "epoch: 401, d_loss: 0.24,         d_acc: 0.97, g_loss: 1.90\n",
      "[' to to to to to to to to the and in a in in of in a in in of of in in of in of of of of of of of in a a a a and to']\n",
      "epoch: 402, d_loss: 0.22,         d_acc: 0.99, g_loss: 2.08\n",
      "[' to to to the the to to to to and a of in our of our our in of of our our of our our of our our our our our our our our a in in in and to']\n",
      "epoch: 403, d_loss: 0.23,         d_acc: 0.98, g_loss: 1.94\n",
      "[' to to to to to to to the and a a in a in in a a in in in in in in in in in in of in in in in and a and a and to']\n",
      "epoch: 404, d_loss: 0.27,         d_acc: 0.96, g_loss: 1.95\n",
      "[' to to to to to to to to the and in a in a of in a in in in of in in in in in of in of of of in in and a a a and to']\n",
      "epoch: 405, d_loss: 0.24,         d_acc: 0.98, g_loss: 1.82\n",
      "[' the to to the the to to the to to a in you of this you it this our you you this it you this this you this it this it this it this this of our of of in to']\n",
      "epoch: 406, d_loss: 0.24,         d_acc: 0.98, g_loss: 1.83\n",
      "[' to to to the the to to to to and a of in our of our our in of of our our of of our of our our our our our our our our a in a in and to']\n",
      "epoch: 407, d_loss: 0.27,         d_acc: 0.97, g_loss: 1.81\n",
      "[' to to to to to to to the and a a in a in in a a in in in in in in a in in in in in in in in and a and and the to']\n",
      "epoch: 408, d_loss: 0.24,         d_acc: 0.99, g_loss: 1.91\n",
      "[' the the the and and to the the to to a of this you for this for it you it it for for it it for it for for for is for for it for our you our you in to']\n",
      "epoch: 409, d_loss: 0.24,         d_acc: 0.99, g_loss: 1.79\n",
      "[' to to to the the to to to to and a of in our of you our of of our our you of our our of our our our you our our our our a in in in and to']\n",
      "epoch: 410, d_loss: 0.26,         d_acc: 0.98, g_loss: 1.71\n",
      "[' to to to the the to to to to and a of in our in our of in of of our our of of our of our our our our our our of of a in a a and to']\n",
      "epoch: 411, d_loss: 0.26,         d_acc: 0.97, g_loss: 1.77\n",
      "[' to to to to to to to to the and in a in in of in a in in of of in in of in of of of of of of in in and a and a the']\n",
      "epoch: 412, d_loss: 0.27,         d_acc: 0.97, g_loss: 1.77\n",
      "[' to to to the the to to to to and a of in our of our our in of of our our of our our of our our our you our our our our a in a in and to']\n",
      "epoch: 413, d_loss: 0.24,         d_acc: 1.00, g_loss: 1.61\n",
      "[' to to to to the to to to to and a of in our in our of in of of our our of of our of our our our our our our of of a in a a and']\n",
      "epoch: 414, d_loss: 0.24,         d_acc: 1.00, g_loss: 1.70\n",
      "[' to to to to to to the and a and in a in in a a a in in a in in a in in in in in in a a and and and and the']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 415, d_loss: 0.28,         d_acc: 0.98, g_loss: 1.62\n",
      "[' to to to to the to to to to and a of in of in our of in of of our our of of our of our our our our our our of of a in a a and']\n",
      "epoch: 416, d_loss: 0.27,         d_acc: 0.98, g_loss: 1.58\n",
      "[' to to to to to to to to the and in a of in of of a in in of of in of of in of of of of of of in of and a and a the']\n",
      "epoch: 417, d_loss: 0.25,         d_acc: 1.00, g_loss: 1.62\n",
      "[' to to to to the to to to to and a of in of in our of in of of our our of of our of our our our our our our of of a in a a and']\n",
      "epoch: 418, d_loss: 0.30,         d_acc: 0.98, g_loss: 1.68\n",
      "[' to to to the the to to the to to and in our of you our this you of you you this this you you you you you this you this you this you you in of in in and']\n",
      "epoch: 419, d_loss: 0.25,         d_acc: 0.99, g_loss: 1.76\n",
      "[' to to to the the to to the to and a of in our of you our of our our you you our our you our our you our you our you our our a in a in and']\n",
      "epoch: 420, d_loss: 0.29,         d_acc: 0.98, g_loss: 1.65\n",
      "[' to to to the the to to to to and a of in our of you our of of our our you of our our of our our our you our our our our a in a a and']\n",
      "epoch: 421, d_loss: 0.28,         d_acc: 0.99, g_loss: 1.55\n",
      "[' to to to to the to to to the and of a of in our of in of of our our of of of of of our of our of of of of a a and a the']\n",
      "epoch: 422, d_loss: 0.29,         d_acc: 0.98, g_loss: 1.37\n",
      "[' to to to to to the and a and in a in in a a a in in a in in a in in in in in in a a and and the and the']\n",
      "epoch: 423, d_loss: 0.33,         d_acc: 0.98, g_loss: 1.56\n",
      "[' to to to to the to to to the a of in of in our of in of of our our of of of of of our of our of of of of and a and a the']\n",
      "epoch: 424, d_loss: 0.31,         d_acc: 0.98, g_loss: 1.55\n",
      "[' to to to to to to to to the and in a in in of in a in in of of in in of in in of in of in in in in and and and and the']\n",
      "epoch: 425, d_loss: 0.29,         d_acc: 0.99, g_loss: 1.47\n",
      "[' to to to to the the a and a a in a a a a in in a a in a in in in in a a a a the and the and to']\n",
      "epoch: 426, d_loss: 0.32,         d_acc: 0.99, g_loss: 1.40\n",
      "[' to to to to to to to the and a a in a in in a in in in in in in in a in in in in in in in in and and the and the']\n",
      "epoch: 427, d_loss: 0.32,         d_acc: 0.99, g_loss: 1.43\n",
      "[' to to to the the to to to to and a of in our of you our in of our our you of our our of our our our our our our of our a a a a the']\n",
      "epoch: 428, d_loss: 0.30,         d_acc: 1.00, g_loss: 1.58\n",
      "[' to to to to to to the and a and in a in in a a a in in a in in a in in in in in in a a the and the and to']\n",
      "epoch: 429, d_loss: 0.33,         d_acc: 0.99, g_loss: 1.44\n",
      "[' to to to to the the a and in a in a a a a in in a a in a in in in in a a a a the and the the to']\n",
      "epoch: 430, d_loss: 0.34,         d_acc: 0.99, g_loss: 1.37\n",
      "[' to to to the the a and a a in a and a a a a a a a a a a a a a a a a the the the the to']\n",
      "epoch: 431, d_loss: 0.35,         d_acc: 0.97, g_loss: 1.46\n",
      "[' to to to to to to to the and a a in a of in a in in in of in in in in in in in in in in a in and and the and to']\n",
      "epoch: 432, d_loss: 0.32,         d_acc: 0.98, g_loss: 1.28\n",
      "[' to to to to to to to the and a a in a in in a in in in in in in in a in in in in in in a in and and the and to']\n",
      "epoch: 433, d_loss: 0.30,         d_acc: 0.99, g_loss: 1.26\n",
      "[' to to to to to to to the and in a in a of in a in in in of in in in in in in in of in in in in and and the and to']\n",
      "epoch: 434, d_loss: 0.37,         d_acc: 0.97, g_loss: 1.31\n",
      "[' to to to to the to to to the and of in of in our of in of of our our of of of of of of of our of of in of and a and and the']\n",
      "epoch: 435, d_loss: 0.35,         d_acc: 0.99, g_loss: 1.29\n",
      "[' to to to to the the a and a a in a a a a in in a a in a in in a in a a a a the the the the to']\n",
      "epoch: 436, d_loss: 0.37,         d_acc: 0.97, g_loss: 1.17\n",
      "[' to to to to the the a and a a in a a a a in in a a in a a in a in a a a a the the the the to']\n",
      "epoch: 437, d_loss: 0.40,         d_acc: 0.97, g_loss: 1.40\n",
      "[' the to to the and to to the to to a in you our this you it this our you this this it you this this you this this this it this this you you a in a in and']\n",
      "epoch: 438, d_loss: 0.36,         d_acc: 0.99, g_loss: 1.24\n",
      "[' to to to to to to the and a and in a in in a a a in in a in in a in in in in a a a a the and the the to']\n",
      "epoch: 439, d_loss: 0.39,         d_acc: 0.97, g_loss: 1.34\n",
      "[' the to the and and to the the to to a of this our it this for it you this it for for this it it this it for it for it it this it in of a in and']\n",
      "epoch: 440, d_loss: 0.37,         d_acc: 0.99, g_loss: 1.24\n",
      "[' to to to to the and and a and a a and and a a a a a a and a a a a a a and and the the to the to']\n",
      "epoch: 441, d_loss: 0.41,         d_acc: 0.97, g_loss: 1.19\n",
      "[' to to to to the the a and a a in a and a a a in a a a a a a a a a a a a the the the the to']\n",
      "epoch: 442, d_loss: 0.42,         d_acc: 0.96, g_loss: 1.13\n",
      "[' to to to to the and and a and a a and a a a a a a a a a a a a a a and a the the to the to']\n",
      "epoch: 443, d_loss: 0.41,         d_acc: 0.96, g_loss: 1.10\n",
      "[' to to to to the and and a and a a and and a a a and a a and a a a a a a and and the the to the to']\n",
      "epoch: 444, d_loss: 0.38,         d_acc: 1.00, g_loss: 1.26\n",
      "[' to to to to to to to the and in a in a of in a in in in of in in in in in in in in in in a in the and the the to']\n",
      "epoch: 445, d_loss: 0.46,         d_acc: 0.98, g_loss: 1.13\n",
      "[' to to to to to to to to the and in a in in of in a in in of of in in in in in of in of in in a in the and the and to']\n",
      "epoch: 446, d_loss: 0.44,         d_acc: 0.97, g_loss: 1.04\n",
      "[' to to to to the and and a and a a and a a a a a a a and a a a a a a and a the the to the to']\n",
      "epoch: 447, d_loss: 0.42,         d_acc: 0.98, g_loss: 1.02\n",
      "[' to to to to the and the and and a and and and and a a and and and and and and and and and and and and to the to to']\n",
      "epoch: 448, d_loss: 0.47,         d_acc: 0.95, g_loss: 1.03\n",
      "[' to to to to the and the and and a and and and and a a and and and and and and and a and and and and to the to to']\n",
      "epoch: 449, d_loss: 0.45,         d_acc: 0.99, g_loss: 1.04\n",
      "[' to to to the the to to the to and a of in our of you our of our our you you our our our of our our our you our our of our and a and and to']\n",
      "epoch: 450, d_loss: 0.41,         d_acc: 1.00, g_loss: 0.87\n",
      "[' to to to to the and the and and and and and and and and and and and and and and and and and and and the and to to to to']\n",
      "epoch: 451, d_loss: 0.44,         d_acc: 0.97, g_loss: 1.02\n",
      "[' to to to the the to to to to and a of in our of our our in of of our our of our our of our our our our of of in of and and the and to']\n",
      "epoch: 452, d_loss: 0.50,         d_acc: 0.95, g_loss: 1.07\n",
      "[' to to to to the and the and and a and and and and a a and and and and and and and and and and and and to to to to']\n",
      "epoch: 453, d_loss: 0.51,         d_acc: 0.95, g_loss: 1.01\n",
      "[' to to to to the the a and a a in a and a a a in a a a a a a a a a a and a the the to the to']\n",
      "epoch: 454, d_loss: 0.44,         d_acc: 0.95, g_loss: 0.94\n",
      "[' to to the and the and and and and the and and and and and and and and and and and and and and the and to to to to']\n",
      "epoch: 455, d_loss: 0.46,         d_acc: 0.95, g_loss: 0.99\n",
      "[' to to to to the and the and and a and and and and and a and and and and and and and and and and the and to to to to']\n",
      "epoch: 456, d_loss: 0.48,         d_acc: 0.96, g_loss: 0.93\n",
      "[' to to the and the and and and and the and and and and and and and and and and and and and and the and to to to to']\n",
      "epoch: 457, d_loss: 0.46,         d_acc: 0.96, g_loss: 0.90\n",
      "[' to to to to the and and a and a a and and and a a and and a and a a and a and and and and to to to to']\n",
      "epoch: 458, d_loss: 0.48,         d_acc: 0.92, g_loss: 0.98\n",
      "[' to to the and the and the and and the and and and and and and and the and and and and and the the the to to to to']\n",
      "epoch: 459, d_loss: 0.47,         d_acc: 0.90, g_loss: 0.93\n",
      "[' to to the the the and the and and the and and and and and and and the and and and and the the the the to to to to']\n",
      "epoch: 460, d_loss: 0.45,         d_acc: 0.92, g_loss: 1.02\n",
      "[' to to to to to to to the and in a in in of in a in in in of in in in in in in in in in in a in the the to the to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 461, d_loss: 0.47,         d_acc: 0.96, g_loss: 0.89\n",
      "[' to to to to the and and a and a a and and a a a and a a and a a a a and and and and to to to to']\n",
      "epoch: 462, d_loss: 0.53,         d_acc: 0.93, g_loss: 0.99\n",
      "[' to to the the the and the and and the the and and and and and and the and and and and the the the the to to to to']\n",
      "epoch: 463, d_loss: 0.46,         d_acc: 0.91, g_loss: 0.90\n",
      "[' to to to to the and and a and a a and and a a a and a a and a a a a and and and and to to to to']\n",
      "epoch: 464, d_loss: 0.47,         d_acc: 0.92, g_loss: 0.93\n",
      "[' to to the the and the and and the the the and and the the and the the and the and the the the the to to to']\n",
      "epoch: 465, d_loss: 0.52,         d_acc: 0.93, g_loss: 0.85\n",
      "[' to to the and the and and and and the and and and and and and and and and and and and and and the and to to to to']\n",
      "epoch: 466, d_loss: 0.50,         d_acc: 0.87, g_loss: 0.90\n",
      "[' to to to to to to the and a a in a in in a a in in in a in in a in in in in a a a a the the to the']\n",
      "epoch: 467, d_loss: 0.50,         d_acc: 0.92, g_loss: 0.96\n",
      "[' to to to the the and the and and the the and and and the and and the and and and and the the the the to to to']\n",
      "epoch: 468, d_loss: 0.50,         d_acc: 0.94, g_loss: 0.88\n",
      "[' to to to to the and and and and a and and and and a a and and and and and and and and and and the and to to to to']\n",
      "epoch: 469, d_loss: 0.52,         d_acc: 0.92, g_loss: 0.94\n",
      "[' to to to the the to to to to and a of in our of our our in of of our our of our our of our our of our of of in of the and the and to']\n",
      "epoch: 470, d_loss: 0.47,         d_acc: 0.92, g_loss: 0.95\n",
      "[' to to to to the and and a and a a and and a a a and a a and a a a a and and and and to to to to']\n",
      "epoch: 471, d_loss: 0.54,         d_acc: 0.90, g_loss: 0.94\n",
      "[' to to to to the to to to the and in a of in our of in of of of our of of of in of of of of in in a in the the to the to']\n",
      "epoch: 472, d_loss: 0.51,         d_acc: 0.90, g_loss: 0.85\n",
      "[' to to the the the the and the the the the and and the the the the the the the the the the the the to to to']\n",
      "epoch: 473, d_loss: 0.46,         d_acc: 0.90, g_loss: 0.99\n",
      "[' to to to to the and and a and a a and a a a a a a a and a a a a and and and and to to to to']\n",
      "epoch: 474, d_loss: 0.52,         d_acc: 0.87, g_loss: 0.92\n",
      "[' to to to to the and and a and a a and a a a a a a a and a a a a and and and and to to to to']\n",
      "epoch: 475, d_loss: 0.48,         d_acc: 0.86, g_loss: 0.87\n",
      "[' to to to to the and and a and a a and and and a a and a a and a a and a and and and and to to to to']\n",
      "epoch: 476, d_loss: 0.53,         d_acc: 0.82, g_loss: 0.86\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the the to the to to']\n",
      "epoch: 477, d_loss: 0.51,         d_acc: 0.85, g_loss: 0.85\n",
      "[' to to the the the the and the the the the the the the the the the the the the the the the to the to to']\n",
      "epoch: 478, d_loss: 0.48,         d_acc: 0.91, g_loss: 0.91\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the the to the to to']\n",
      "epoch: 479, d_loss: 0.40,         d_acc: 0.89, g_loss: 0.90\n",
      "[' to to to to the and a and in a in in a a a in in a a in a a a a in a a and a to to to to']\n",
      "epoch: 480, d_loss: 0.47,         d_acc: 0.85, g_loss: 0.89\n",
      "[' to to the the and the and and the the the and and the the and the the the the and the the the the to to to']\n",
      "epoch: 481, d_loss: 0.49,         d_acc: 0.84, g_loss: 0.89\n",
      "[' to to the the the the and the the the the the the the the the the the the the the the the to the to to']\n",
      "epoch: 482, d_loss: 0.50,         d_acc: 0.79, g_loss: 0.87\n",
      "[' to to to the the and the and and the the the and and the and and the and and the and the the the the to to to']\n",
      "epoch: 483, d_loss: 0.49,         d_acc: 0.74, g_loss: 0.93\n",
      "[' to to the the the the and the the the the the and the the the the the the the the the the to the to to']\n",
      "epoch: 484, d_loss: 0.46,         d_acc: 0.76, g_loss: 0.89\n",
      "[' to to to to the the a and a a in a and a a in in a a a a a a a a a a and a to to to to']\n",
      "epoch: 485, d_loss: 0.50,         d_acc: 0.74, g_loss: 0.96\n",
      "[' to to to to the the a and a a in a and a a a a a a a a a a a a and and and and to to to to']\n",
      "epoch: 486, d_loss: 0.52,         d_acc: 0.71, g_loss: 0.85\n",
      "[' to to the the the the and the the the the the and the the the the the the the the the the the the to to']\n",
      "epoch: 487, d_loss: 0.44,         d_acc: 0.81, g_loss: 0.90\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the the to the to']\n",
      "epoch: 488, d_loss: 0.47,         d_acc: 0.74, g_loss: 0.87\n",
      "[' to to to to the and and a and a a and and and a a and and and and and and and and and and the and to to to']\n",
      "epoch: 489, d_loss: 0.48,         d_acc: 0.79, g_loss: 0.91\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the the to the']\n",
      "epoch: 490, d_loss: 0.55,         d_acc: 0.68, g_loss: 0.94\n",
      "[' to to the to the the the the the the the the the the the the the the the the the the to to to']\n",
      "epoch: 491, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.87\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the the to the']\n",
      "epoch: 492, d_loss: 0.51,         d_acc: 0.75, g_loss: 0.97\n",
      "[' to to to to to to to the and a a in a in in a a in in in a in in a in in a in a a and a to to to to']\n",
      "epoch: 493, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.93\n",
      "[' to to to to the to to to the and of in of in our of in of of our our of of of in of of of of in in a in the the to the']\n",
      "epoch: 494, d_loss: 0.54,         d_acc: 0.70, g_loss: 0.89\n",
      "[' to to to to the to to to the and in in of in our of in of of of our of of of in of of of of in in a in the the to the']\n",
      "epoch: 495, d_loss: 0.51,         d_acc: 0.71, g_loss: 0.94\n",
      "[' to to the to the to the the to the the the the the the the to the the the the to to to to']\n",
      "epoch: 496, d_loss: 0.53,         d_acc: 0.73, g_loss: 0.89\n",
      "[' to to to to the and the and and a and and and and a a and and and and and and and and and and the and to to to']\n",
      "epoch: 497, d_loss: 0.43,         d_acc: 0.82, g_loss: 0.89\n",
      "[' to to the the the the and the the the the the the the the the the the the the the the the to the']\n",
      "epoch: 498, d_loss: 0.54,         d_acc: 0.71, g_loss: 0.91\n",
      "[' to to to the to the the to to to the the to to the to to to to to to to to to']\n",
      "epoch: 499, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.87\n",
      "[' to to to to the and the and and and and the and and and and and and and and and and and and the the the the to to to']\n",
      "epoch: 500, d_loss: 0.51,         d_acc: 0.74, g_loss: 0.89\n",
      "[' to to to to the the a and a a in a and a a a in a a a a a a a a and and and and to to to']\n",
      "epoch: 501, d_loss: 0.53,         d_acc: 0.70, g_loss: 0.86\n",
      "[' to to to to to the to to to to to the to to to to to to to to to to to to']\n",
      "epoch: 502, d_loss: 0.43,         d_acc: 0.82, g_loss: 0.90\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 503, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.87\n",
      "[' to to to to the the a and a a in a and a a in in a a a a a a a a a and and and to to to']\n",
      "epoch: 504, d_loss: 0.48,         d_acc: 0.78, g_loss: 0.87\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 505, d_loss: 0.52,         d_acc: 0.75, g_loss: 0.90\n",
      "[' to to to the to the the to to the the the to the the to the the to to to to to to']\n",
      "epoch: 506, d_loss: 0.51,         d_acc: 0.73, g_loss: 0.89\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 507, d_loss: 0.49,         d_acc: 0.79, g_loss: 0.89\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 508, d_loss: 0.60,         d_acc: 0.67, g_loss: 0.90\n",
      "[' to to the the the and the and and the and and and and and and and the and and and and the the the the to']\n",
      "epoch: 509, d_loss: 0.55,         d_acc: 0.72, g_loss: 0.89\n",
      "[' to to to the to the the to to to the the to to to to to to to to to to to to']\n",
      "epoch: 510, d_loss: 0.54,         d_acc: 0.71, g_loss: 0.87\n",
      "[' to to to to to to to']\n",
      "epoch: 511, d_loss: 0.51,         d_acc: 0.76, g_loss: 0.88\n",
      "['']\n",
      "epoch: 512, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.85\n",
      "[' to to to to the to the the to to the the the to the the to the the to the to to to to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 513, d_loss: 0.52,         d_acc: 0.76, g_loss: 0.88\n",
      "[' to to to to to to to to to to']\n",
      "epoch: 514, d_loss: 0.55,         d_acc: 0.73, g_loss: 0.84\n",
      "['']\n",
      "epoch: 515, d_loss: 0.53,         d_acc: 0.73, g_loss: 0.85\n",
      "[' to to the the and the and and the the the and and the the and the the the the the the the the the to']\n",
      "epoch: 516, d_loss: 0.51,         d_acc: 0.79, g_loss: 0.87\n",
      "['']\n",
      "epoch: 517, d_loss: 0.55,         d_acc: 0.72, g_loss: 0.84\n",
      "['']\n",
      "epoch: 518, d_loss: 0.55,         d_acc: 0.74, g_loss: 0.85\n",
      "[' to to to to to']\n",
      "epoch: 519, d_loss: 0.50,         d_acc: 0.76, g_loss: 0.84\n",
      "[' to to to to to the to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 520, d_loss: 0.57,         d_acc: 0.70, g_loss: 0.85\n",
      "[' to to to the to the the to to to the the to to to to to to to to to to to to']\n",
      "epoch: 521, d_loss: 0.50,         d_acc: 0.80, g_loss: 0.84\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 522, d_loss: 0.54,         d_acc: 0.72, g_loss: 0.85\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 523, d_loss: 0.51,         d_acc: 0.77, g_loss: 0.86\n",
      "['']\n",
      "epoch: 524, d_loss: 0.53,         d_acc: 0.77, g_loss: 0.86\n",
      "['']\n",
      "epoch: 525, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.85\n",
      "['']\n",
      "epoch: 526, d_loss: 0.51,         d_acc: 0.78, g_loss: 0.87\n",
      "['']\n",
      "epoch: 527, d_loss: 0.52,         d_acc: 0.75, g_loss: 0.88\n",
      "['']\n",
      "epoch: 528, d_loss: 0.52,         d_acc: 0.78, g_loss: 0.87\n",
      "['']\n",
      "epoch: 529, d_loss: 0.51,         d_acc: 0.78, g_loss: 0.88\n",
      "['']\n",
      "epoch: 530, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.87\n",
      "['']\n",
      "epoch: 531, d_loss: 0.59,         d_acc: 0.71, g_loss: 0.88\n",
      "['']\n",
      "epoch: 532, d_loss: 218.38,         d_acc: 0.67, g_loss: 0.93\n",
      "['']\n",
      "epoch: 533, d_loss: 0.60,         d_acc: 0.72, g_loss: 0.88\n",
      "[' the the the']\n",
      "epoch: 534, d_loss: 0.77,         d_acc: 0.79, g_loss: 1.02\n",
      "['']\n",
      "epoch: 535, d_loss: 0.54,         d_acc: 0.74, g_loss: 1.01\n",
      "['']\n",
      "epoch: 536, d_loss: 0.63,         d_acc: 0.66, g_loss: 1.09\n",
      "['']\n",
      "epoch: 537, d_loss: 0.51,         d_acc: 0.75, g_loss: 1.29\n",
      "['']\n",
      "epoch: 538, d_loss: 6.56,         d_acc: 0.75, g_loss: 1.10\n",
      "[' you you to to and to']\n",
      "epoch: 539, d_loss: 2.94,         d_acc: 0.67, g_loss: 1.09\n",
      "['']\n",
      "epoch: 540, d_loss: 0.66,         d_acc: 0.72, g_loss: 1.67\n",
      "['']\n",
      "epoch: 541, d_loss: 0.69,         d_acc: 0.68, g_loss: 15.26\n",
      "[' vote is and to a the to']\n",
      "epoch: 542, d_loss: 1.00,         d_acc: 0.64, g_loss: 1.23\n",
      "[' to']\n",
      "epoch: 543, d_loss: 0.78,         d_acc: 0.68, g_loss: 8.54\n",
      "['']\n",
      "epoch: 544, d_loss: 0.66,         d_acc: 0.63, g_loss: 1.10\n",
      "['']\n",
      "epoch: 545, d_loss: 0.83,         d_acc: 0.66, g_loss: 1.02\n",
      "['']\n",
      "epoch: 546, d_loss: 0.70,         d_acc: 0.68, g_loss: 24.85\n",
      "['']\n",
      "epoch: 547, d_loss: 0.62,         d_acc: 0.66, g_loss: 1.90\n",
      "['']\n",
      "epoch: 548, d_loss: 0.69,         d_acc: 0.71, g_loss: 0.93\n",
      "['']\n",
      "epoch: 549, d_loss: 0.77,         d_acc: 0.65, g_loss: 0.91\n",
      "['']\n",
      "epoch: 550, d_loss: 0.63,         d_acc: 0.72, g_loss: 0.89\n",
      "['']\n",
      "epoch: 551, d_loss: 0.60,         d_acc: 0.72, g_loss: 0.88\n",
      "['']\n",
      "epoch: 552, d_loss: 0.61,         d_acc: 0.73, g_loss: 0.86\n",
      "['']\n",
      "epoch: 553, d_loss: 0.62,         d_acc: 0.68, g_loss: 0.84\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 554, d_loss: 0.60,         d_acc: 0.71, g_loss: 0.84\n",
      "['']\n",
      "epoch: 555, d_loss: 0.63,         d_acc: 0.72, g_loss: 0.82\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 556, d_loss: 0.67,         d_acc: 0.68, g_loss: 0.81\n",
      "['']\n",
      "epoch: 557, d_loss: 0.56,         d_acc: 0.75, g_loss: 0.80\n",
      "['']\n",
      "epoch: 558, d_loss: 0.59,         d_acc: 0.70, g_loss: 0.80\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 559, d_loss: 0.50,         d_acc: 0.80, g_loss: 0.81\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 560, d_loss: 0.57,         d_acc: 0.73, g_loss: 0.79\n",
      "['']\n",
      "epoch: 561, d_loss: 0.56,         d_acc: 0.74, g_loss: 0.81\n",
      "['']\n",
      "epoch: 562, d_loss: 0.57,         d_acc: 0.75, g_loss: 0.81\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 563, d_loss: 0.53,         d_acc: 0.78, g_loss: 0.80\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 564, d_loss: 0.57,         d_acc: 0.73, g_loss: 0.81\n",
      "[' to to to to to the to to to to to the to to to to to to to to to to to to']\n",
      "epoch: 565, d_loss: 0.56,         d_acc: 0.76, g_loss: 0.81\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 566, d_loss: 0.58,         d_acc: 0.72, g_loss: 0.82\n",
      "['']\n",
      "epoch: 567, d_loss: 0.57,         d_acc: 0.75, g_loss: 0.82\n",
      "[' to to the to the the the the the the the the the the the the the the the the the to to to to']\n",
      "epoch: 568, d_loss: 0.52,         d_acc: 0.79, g_loss: 0.83\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 569, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.82\n",
      "['']\n",
      "epoch: 570, d_loss: 0.56,         d_acc: 0.72, g_loss: 0.83\n",
      "['']\n",
      "epoch: 571, d_loss: 0.53,         d_acc: 0.79, g_loss: 0.84\n",
      "[' to to the the the the and the the the the and and the the the the the the the the the the to the']\n",
      "epoch: 572, d_loss: 0.54,         d_acc: 0.76, g_loss: 0.86\n",
      "['']\n",
      "epoch: 573, d_loss: 0.50,         d_acc: 0.78, g_loss: 0.85\n",
      "['']\n",
      "epoch: 574, d_loss: 0.49,         d_acc: 0.82, g_loss: 0.86\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 575, d_loss: 0.54,         d_acc: 0.75, g_loss: 0.87\n",
      "['']\n",
      "epoch: 576, d_loss: 0.54,         d_acc: 0.71, g_loss: 0.88\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 577, d_loss: 0.55,         d_acc: 0.74, g_loss: 0.88\n",
      "[' to to to to the and the and and and and the and and and and and and and and and and and and the the the the to']\n",
      "epoch: 578, d_loss: 0.50,         d_acc: 0.78, g_loss: 0.89\n",
      "['']\n",
      "epoch: 579, d_loss: 0.59,         d_acc: 0.68, g_loss: 0.89\n",
      "['']\n",
      "epoch: 580, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.90\n",
      "['']\n",
      "epoch: 581, d_loss: 0.55,         d_acc: 0.72, g_loss: 0.91\n",
      "['']\n",
      "epoch: 582, d_loss: 0.55,         d_acc: 0.71, g_loss: 0.90\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 583, d_loss: 0.56,         d_acc: 0.72, g_loss: 0.91\n",
      "['']\n",
      "epoch: 584, d_loss: 0.51,         d_acc: 0.75, g_loss: 0.92\n",
      "[' to to the to the the the the the the the the the the the the the the the the the to to to to']\n",
      "epoch: 585, d_loss: 0.54,         d_acc: 0.71, g_loss: 0.92\n",
      "[' to to to to']\n",
      "epoch: 586, d_loss: 0.51,         d_acc: 0.76, g_loss: 0.91\n",
      "['']\n",
      "epoch: 587, d_loss: 0.50,         d_acc: 0.76, g_loss: 0.92\n",
      "[' to to to to to the to to to to to the to to to to to to to to to to to to']\n",
      "epoch: 588, d_loss: 0.52,         d_acc: 0.74, g_loss: 0.91\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 589, d_loss: 0.58,         d_acc: 0.68, g_loss: 0.93\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 590, d_loss: 0.47,         d_acc: 0.80, g_loss: 0.93\n",
      "['']\n",
      "epoch: 591, d_loss: 0.53,         d_acc: 0.73, g_loss: 0.93\n",
      "['']\n",
      "epoch: 592, d_loss: 0.54,         d_acc: 0.73, g_loss: 0.93\n",
      "['']\n",
      "epoch: 593, d_loss: 0.55,         d_acc: 0.70, g_loss: 0.94\n",
      "[' to to to to to the to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 594, d_loss: 0.50,         d_acc: 0.77, g_loss: 0.94\n",
      "['']\n",
      "epoch: 595, d_loss: 0.48,         d_acc: 0.82, g_loss: 0.93\n",
      "['']\n",
      "epoch: 596, d_loss: 0.54,         d_acc: 0.73, g_loss: 0.93\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 597, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.94\n",
      "['']\n",
      "epoch: 598, d_loss: 0.51,         d_acc: 0.74, g_loss: 0.95\n",
      "[' to to to to to']\n",
      "epoch: 599, d_loss: 0.54,         d_acc: 0.73, g_loss: 0.95\n",
      "[' to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 600, d_loss: 0.55,         d_acc: 0.73, g_loss: 0.95\n",
      "['']\n",
      "epoch: 601, d_loss: 0.56,         d_acc: 0.73, g_loss: 0.94\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 602, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.93\n",
      "[' to']\n",
      "epoch: 603, d_loss: 0.53,         d_acc: 0.74, g_loss: 0.95\n",
      "['']\n",
      "epoch: 604, d_loss: 0.54,         d_acc: 0.72, g_loss: 0.94\n",
      "['']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 605, d_loss: 0.47,         d_acc: 0.81, g_loss: 0.95\n",
      "['']\n",
      "epoch: 606, d_loss: 0.52,         d_acc: 0.73, g_loss: 0.95\n",
      "['']\n",
      "epoch: 607, d_loss: 0.57,         d_acc: 0.66, g_loss: 0.94\n",
      "[' to to to the to the the to to to the the to to to to to to to to to to to to']\n",
      "epoch: 608, d_loss: 0.54,         d_acc: 0.73, g_loss: 0.95\n",
      "['']\n",
      "epoch: 609, d_loss: 0.53,         d_acc: 0.70, g_loss: 0.94\n",
      "['']\n",
      "epoch: 610, d_loss: 0.51,         d_acc: 0.75, g_loss: 0.94\n",
      "['']\n",
      "epoch: 611, d_loss: 0.49,         d_acc: 0.78, g_loss: 0.94\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 612, d_loss: 0.50,         d_acc: 0.74, g_loss: 0.94\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 613, d_loss: 0.52,         d_acc: 0.73, g_loss: 0.94\n",
      "['']\n",
      "epoch: 614, d_loss: 0.60,         d_acc: 0.67, g_loss: 0.94\n",
      "['']\n",
      "epoch: 615, d_loss: 0.53,         d_acc: 0.72, g_loss: 0.94\n",
      "[' to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 616, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.94\n",
      "['']\n",
      "epoch: 617, d_loss: 0.51,         d_acc: 0.76, g_loss: 0.94\n",
      "[' to to to to to']\n",
      "epoch: 618, d_loss: 0.47,         d_acc: 0.81, g_loss: 0.95\n",
      "['']\n",
      "epoch: 619, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.94\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 620, d_loss: 0.51,         d_acc: 0.75, g_loss: 0.96\n",
      "['']\n",
      "epoch: 621, d_loss: 0.46,         d_acc: 0.82, g_loss: 0.95\n",
      "[' to to the the the and the and and the and and and and the and and the and and the and the the the the']\n",
      "epoch: 622, d_loss: 0.55,         d_acc: 0.71, g_loss: 0.95\n",
      "[' to to to to the and the and and a and and and and and and and and and and and and and and and the the the to']\n",
      "epoch: 623, d_loss: 0.50,         d_acc: 0.78, g_loss: 0.96\n",
      "['']\n",
      "epoch: 624, d_loss: 0.51,         d_acc: 0.74, g_loss: 0.96\n",
      "['']\n",
      "epoch: 625, d_loss: 0.52,         d_acc: 0.74, g_loss: 0.96\n",
      "[' to to to to to to to to to to']\n",
      "epoch: 626, d_loss: 0.56,         d_acc: 0.69, g_loss: 0.96\n",
      "[' to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 627, d_loss: 0.56,         d_acc: 0.70, g_loss: 0.96\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the the to the']\n",
      "epoch: 628, d_loss: 0.49,         d_acc: 0.77, g_loss: 0.95\n",
      "['']\n",
      "epoch: 629, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.96\n",
      "['']\n",
      "epoch: 630, d_loss: 0.52,         d_acc: 0.74, g_loss: 0.96\n",
      "[' to to the the the the and the the the the the and the the the the the the the the the the to the']\n",
      "epoch: 631, d_loss: 0.52,         d_acc: 0.72, g_loss: 0.96\n",
      "['']\n",
      "epoch: 632, d_loss: 0.53,         d_acc: 0.72, g_loss: 0.95\n",
      "['']\n",
      "epoch: 633, d_loss: 0.52,         d_acc: 0.75, g_loss: 0.96\n",
      "['']\n",
      "epoch: 634, d_loss: 0.53,         d_acc: 0.75, g_loss: 0.95\n",
      "['']\n",
      "epoch: 635, d_loss: 0.52,         d_acc: 0.74, g_loss: 0.96\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 636, d_loss: 0.54,         d_acc: 0.72, g_loss: 0.95\n",
      "['']\n",
      "epoch: 637, d_loss: 0.53,         d_acc: 0.74, g_loss: 0.95\n",
      "['']\n",
      "epoch: 638, d_loss: 0.53,         d_acc: 0.73, g_loss: 0.95\n",
      "[' to to to the to the the to to the the the to the the to the the to to to to to to']\n",
      "epoch: 639, d_loss: 0.52,         d_acc: 0.73, g_loss: 0.95\n",
      "['']\n",
      "epoch: 640, d_loss: 0.53,         d_acc: 0.74, g_loss: 0.95\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 641, d_loss: 0.49,         d_acc: 0.77, g_loss: 0.94\n",
      "['']\n",
      "epoch: 642, d_loss: 0.48,         d_acc: 0.76, g_loss: 0.94\n",
      "[' to to the and the and and and and the and and and and and and and and and and and and the the the the']\n",
      "epoch: 643, d_loss: 0.55,         d_acc: 0.68, g_loss: 0.95\n",
      "['']\n",
      "epoch: 644, d_loss: 0.48,         d_acc: 0.75, g_loss: 0.95\n",
      "[' to']\n",
      "epoch: 645, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.96\n",
      "[' to to the the the the the the the the the the the the the the the the the the the the to to to']\n",
      "epoch: 646, d_loss: 0.49,         d_acc: 0.77, g_loss: 0.95\n",
      "['']\n",
      "epoch: 647, d_loss: 0.52,         d_acc: 0.72, g_loss: 0.96\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 648, d_loss: 0.51,         d_acc: 0.75, g_loss: 0.95\n",
      "['']\n",
      "epoch: 649, d_loss: 0.55,         d_acc: 0.70, g_loss: 0.95\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 650, d_loss: 0.51,         d_acc: 0.75, g_loss: 0.96\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 651, d_loss: 0.48,         d_acc: 0.78, g_loss: 0.96\n",
      "[' to to the the the the and the the the the the the the the the the the the the the the the to the']\n",
      "epoch: 652, d_loss: 0.55,         d_acc: 0.68, g_loss: 0.96\n",
      "[' to to to to to']\n",
      "epoch: 653, d_loss: 0.45,         d_acc: 0.79, g_loss: 0.96\n",
      "['']\n",
      "epoch: 654, d_loss: 0.53,         d_acc: 0.69, g_loss: 0.96\n",
      "['']\n",
      "epoch: 655, d_loss: 0.49,         d_acc: 0.76, g_loss: 0.97\n",
      "[' to to to the to the the to to the the the to the the to the to to to to to to to']\n",
      "epoch: 656, d_loss: 0.51,         d_acc: 0.72, g_loss: 0.97\n",
      "['']\n",
      "epoch: 657, d_loss: 0.46,         d_acc: 0.79, g_loss: 0.97\n",
      "['']\n",
      "epoch: 658, d_loss: 0.46,         d_acc: 0.78, g_loss: 0.97\n",
      "['']\n",
      "epoch: 659, d_loss: 0.49,         d_acc: 0.78, g_loss: 0.98\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 660, d_loss: 0.48,         d_acc: 0.78, g_loss: 0.98\n",
      "[' to to to to']\n",
      "epoch: 661, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.99\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 662, d_loss: 0.52,         d_acc: 0.75, g_loss: 0.99\n",
      "['']\n",
      "epoch: 663, d_loss: 0.50,         d_acc: 0.75, g_loss: 0.99\n",
      "['']\n",
      "epoch: 664, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.00\n",
      "[' to to to to to']\n",
      "epoch: 665, d_loss: 0.49,         d_acc: 0.75, g_loss: 0.99\n",
      "[' to to to to the to the the to the the the the to the the to the the to the to to to to']\n",
      "epoch: 666, d_loss: 0.51,         d_acc: 0.77, g_loss: 0.99\n",
      "['']\n",
      "epoch: 667, d_loss: 0.48,         d_acc: 0.78, g_loss: 0.99\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 668, d_loss: 0.49,         d_acc: 0.75, g_loss: 1.00\n",
      "['']\n",
      "epoch: 669, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.00\n",
      "[' to to to to to the to to to to the the to to to to to to to to to to to to']\n",
      "epoch: 670, d_loss: 0.53,         d_acc: 0.72, g_loss: 1.00\n",
      "['']\n",
      "epoch: 671, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.00\n",
      "[' to to to to to the to to to to the the to to to to to to to to to to to to']\n",
      "epoch: 672, d_loss: 0.52,         d_acc: 0.75, g_loss: 1.01\n",
      "[' to to']\n",
      "epoch: 673, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 674, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.00\n",
      "['']\n",
      "epoch: 675, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to']\n",
      "epoch: 676, d_loss: 0.50,         d_acc: 0.76, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 677, d_loss: 0.45,         d_acc: 0.78, g_loss: 1.01\n",
      "['']\n",
      "epoch: 678, d_loss: 0.56,         d_acc: 0.68, g_loss: 1.01\n",
      "['']\n",
      "epoch: 679, d_loss: 0.50,         d_acc: 0.76, g_loss: 1.00\n",
      "['']\n",
      "epoch: 680, d_loss: 0.50,         d_acc: 0.76, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 681, d_loss: 0.49,         d_acc: 0.76, g_loss: 1.01\n",
      "['']\n",
      "epoch: 682, d_loss: 0.47,         d_acc: 0.79, g_loss: 1.00\n",
      "[' to to to to to']\n",
      "epoch: 683, d_loss: 0.51,         d_acc: 0.75, g_loss: 1.01\n",
      "[' to to to the to the the to to to the the to to the to to to to to to to to to']\n",
      "epoch: 684, d_loss: 0.52,         d_acc: 0.73, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 685, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 686, d_loss: 0.52,         d_acc: 0.71, g_loss: 1.00\n",
      "['']\n",
      "epoch: 687, d_loss: 0.51,         d_acc: 0.74, g_loss: 1.00\n",
      "['']\n",
      "epoch: 688, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 689, d_loss: 0.51,         d_acc: 0.73, g_loss: 1.00\n",
      "[' to to the to the the the the to the the the the the the the the the the the the to to to to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 690, d_loss: 0.49,         d_acc: 0.76, g_loss: 1.00\n",
      "[' to to to the to the the to to the the the to the the to the the to the to to to to']\n",
      "epoch: 691, d_loss: 0.49,         d_acc: 0.75, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 692, d_loss: 0.44,         d_acc: 0.80, g_loss: 1.01\n",
      "['']\n",
      "epoch: 693, d_loss: 0.49,         d_acc: 0.78, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 694, d_loss: 0.45,         d_acc: 0.81, g_loss: 1.01\n",
      "['']\n",
      "epoch: 695, d_loss: 0.52,         d_acc: 0.71, g_loss: 1.00\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 696, d_loss: 0.49,         d_acc: 0.75, g_loss: 1.02\n",
      "['']\n",
      "epoch: 697, d_loss: 0.51,         d_acc: 0.73, g_loss: 1.01\n",
      "[' to to to the the and the and and the the and and and the and and the and and the and the the to the']\n",
      "epoch: 698, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.01\n",
      "['']\n",
      "epoch: 699, d_loss: 0.56,         d_acc: 0.68, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 700, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 701, d_loss: 0.48,         d_acc: 0.79, g_loss: 1.00\n",
      "[' to to to to the the a and a a in a and a a a a a a a a a a a a and and the and to to']\n",
      "epoch: 702, d_loss: 0.50,         d_acc: 0.76, g_loss: 1.01\n",
      "['']\n",
      "epoch: 703, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 704, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to']\n",
      "epoch: 705, d_loss: 0.46,         d_acc: 0.79, g_loss: 1.01\n",
      "['']\n",
      "epoch: 706, d_loss: 0.46,         d_acc: 0.79, g_loss: 1.02\n",
      "[' to to to to to']\n",
      "epoch: 707, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.01\n",
      "[' to to to to']\n",
      "epoch: 708, d_loss: 0.46,         d_acc: 0.80, g_loss: 1.01\n",
      "['']\n",
      "epoch: 709, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.01\n",
      "['']\n",
      "epoch: 710, d_loss: 0.44,         d_acc: 0.81, g_loss: 1.02\n",
      "['']\n",
      "epoch: 711, d_loss: 0.54,         d_acc: 0.69, g_loss: 1.01\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 712, d_loss: 0.53,         d_acc: 0.71, g_loss: 1.01\n",
      "['']\n",
      "epoch: 713, d_loss: 0.46,         d_acc: 0.79, g_loss: 1.02\n",
      "['']\n",
      "epoch: 714, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.01\n",
      "['']\n",
      "epoch: 715, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.01\n",
      "['']\n",
      "epoch: 716, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.02\n",
      "[' to to to to to the to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 717, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.02\n",
      "['']\n",
      "epoch: 718, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.02\n",
      "['']\n",
      "epoch: 719, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.02\n",
      "[' to to to to to to to to']\n",
      "epoch: 720, d_loss: 0.47,         d_acc: 0.77, g_loss: 1.02\n",
      "['']\n",
      "epoch: 721, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.02\n",
      "[' to to the the the the and the the the the the and the the the the the the the the the the to the']\n",
      "epoch: 722, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.03\n",
      "['']\n",
      "epoch: 723, d_loss: 0.52,         d_acc: 0.74, g_loss: 1.03\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 724, d_loss: 0.51,         d_acc: 0.72, g_loss: 1.03\n",
      "[' to to to the to the the to to the the the to the the to the the to the to to to to']\n",
      "epoch: 725, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.03\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 726, d_loss: 0.51,         d_acc: 0.72, g_loss: 1.03\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 727, d_loss: 0.46,         d_acc: 0.80, g_loss: 1.03\n",
      "['']\n",
      "epoch: 728, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.03\n",
      "[' to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 729, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.03\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 730, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.03\n",
      "[' to to the the the the and the the the the and and the the the the the the the the the the to the']\n",
      "epoch: 731, d_loss: 0.49,         d_acc: 0.75, g_loss: 1.03\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 732, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.03\n",
      "['']\n",
      "epoch: 733, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.03\n",
      "['']\n",
      "epoch: 734, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.04\n",
      "[' to to the to the the the the the the the the the the the the the the the the the to to to to']\n",
      "epoch: 735, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.04\n",
      "['']\n",
      "epoch: 736, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.04\n",
      "[' to to the to the to the the to the the the the the the the to the the to the to to to to']\n",
      "epoch: 737, d_loss: 0.48,         d_acc: 0.76, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 738, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.05\n",
      "['']\n",
      "epoch: 739, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.05\n",
      "['']\n",
      "epoch: 740, d_loss: 0.51,         d_acc: 0.71, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 741, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 742, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.05\n",
      "[' to to the to the the the the to the the the the the the the the the the the the to to to to']\n",
      "epoch: 743, d_loss: 0.54,         d_acc: 0.71, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 744, d_loss: 0.49,         d_acc: 0.77, g_loss: 1.04\n",
      "[' to to to to to']\n",
      "epoch: 745, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 746, d_loss: 0.46,         d_acc: 0.78, g_loss: 1.05\n",
      "[' to to to the to the the to to to the the to to the to to to to to to to to to']\n",
      "epoch: 747, d_loss: 0.47,         d_acc: 0.80, g_loss: 1.05\n",
      "['']\n",
      "epoch: 748, d_loss: 0.47,         d_acc: 0.77, g_loss: 1.05\n",
      "[' to to to to to the to to to to the the to to to to to to to to to to to to']\n",
      "epoch: 749, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.05\n",
      "['']\n",
      "epoch: 750, d_loss: 0.42,         d_acc: 0.82, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 751, d_loss: 0.51,         d_acc: 0.73, g_loss: 1.05\n",
      "[' to']\n",
      "epoch: 752, d_loss: 0.43,         d_acc: 0.81, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 753, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.05\n",
      "[' to to to to']\n",
      "epoch: 754, d_loss: 0.41,         d_acc: 0.83, g_loss: 1.06\n",
      "['']\n",
      "epoch: 755, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.06\n",
      "[' to']\n",
      "epoch: 756, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.07\n",
      "['']\n",
      "epoch: 757, d_loss: 0.46,         d_acc: 0.77, g_loss: 1.06\n",
      "['']\n",
      "epoch: 758, d_loss: 0.50,         d_acc: 0.74, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 759, d_loss: 0.48,         d_acc: 0.78, g_loss: 1.06\n",
      "['']\n",
      "epoch: 760, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.06\n",
      "[' to']\n",
      "epoch: 761, d_loss: 0.52,         d_acc: 0.68, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 762, d_loss: 0.44,         d_acc: 0.82, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 763, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to']\n",
      "epoch: 764, d_loss: 0.43,         d_acc: 0.83, g_loss: 1.05\n",
      "['']\n",
      "epoch: 765, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 766, d_loss: 0.44,         d_acc: 0.79, g_loss: 1.06\n",
      "[' to to to the to the to to to to the the to to to to to to to to to to to to']\n",
      "epoch: 767, d_loss: 0.45,         d_acc: 0.79, g_loss: 1.07\n",
      "['']\n",
      "epoch: 768, d_loss: 0.47,         d_acc: 0.75, g_loss: 1.07\n",
      "['']\n",
      "epoch: 769, d_loss: 0.47,         d_acc: 0.76, g_loss: 1.06\n",
      "[' to to to to to to to to to']\n",
      "epoch: 770, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 771, d_loss: 0.49,         d_acc: 0.74, g_loss: 1.07\n",
      "[' to to to to to to to to to to to to to to to']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 772, d_loss: 0.50,         d_acc: 0.75, g_loss: 1.07\n",
      "[' to to the the the the and the the the the the and the the the the the the the the the the to the']\n",
      "epoch: 773, d_loss: 0.47,         d_acc: 0.79, g_loss: 1.06\n",
      "[' to to to to']\n",
      "epoch: 774, d_loss: 0.51,         d_acc: 0.73, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 775, d_loss: 0.43,         d_acc: 0.82, g_loss: 1.06\n",
      "[' to to to to to the to to to to the the to to to to to to to to to to to to']\n",
      "epoch: 776, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.05\n",
      "[' to to the the the the and the the the the the the the the the the the the the the the the to to']\n",
      "epoch: 777, d_loss: 0.47,         d_acc: 0.78, g_loss: 1.06\n",
      "[' to to to to']\n",
      "epoch: 778, d_loss: 0.45,         d_acc: 0.82, g_loss: 1.06\n",
      "[' to to to to to to to to to to to to to to to to to to to to to to to']\n",
      "epoch: 779, d_loss: 0.48,         d_acc: 0.75, g_loss: 1.06\n",
      "[' to to the the and the and and the the the and and the the the the the the the the the the to the']\n",
      "epoch: 780, d_loss: 0.48,         d_acc: 0.77, g_loss: 1.05\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n"
     ]
    }
   ],
   "source": [
    "# Store the losses\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "sample_period = 100 # every `sample_period` steps generate and save some data\"\n",
    "Epoch=3000\n",
    "#x_real= generate_real_samples()\n",
    "######################## Main training loop###########################\n",
    "for i in range(Epoch):\n",
    "        ########################\n",
    "        ###Train Discriminator##\n",
    "        #######################\n",
    "        # prepare real samples\n",
    "        [X_train,y_train]= generate_real_samples()\n",
    "        #print(X_train.shape)\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        #print(idx)\n",
    "        tweets, labels = X_train[idx], y_train[idx]\n",
    "        #print(tweets.shape)\n",
    "        #print(labels.shape)\n",
    "        # Sample noise as generator input\n",
    "        noise = np.random.normal(0, 1, (batch_size, 1))\n",
    "        #print(noise.shape)\n",
    "        # Generate fake tweets\n",
    "        gen_twt= generator.predict([noise,labels])\n",
    "        \n",
    "        \n",
    "        # Train the discriminator\n",
    "        # both loss and accuracy are returned\n",
    "        d_loss_real, d_acc_real=discriminator.train_on_batch([tweets,labels], valid)\n",
    "        d_loss_fake, d_acc_fake =discriminator.train_on_batch([gen_twt,labels], fake)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "        d_acc  = 0.5 * (d_acc_real + d_acc_fake)\n",
    "        \n",
    "        #######################\n",
    "        ### Train generator ###\n",
    "        #######################\n",
    "    \n",
    "        # Condition on labels\n",
    "        sampled_labels = np.random.randint(0, 10, batch_size)                          \n",
    "        #print(sampled_labels.shape)                            \n",
    "        y_gan = ones((57, 1))\n",
    "        # update the generator via the discriminator's error\n",
    "        noise=random.randn(57,1)\n",
    "        #gan_model([noise,x_real[-1]], y_gan).summary()\n",
    "        g_loss, n=gan_model.train_on_batch([noise,sampled_labels], y_gan)\n",
    "        \n",
    "        # Save the losses\n",
    "        d_losses.append(d_loss)\n",
    "        g_losses.append(g_loss)\n",
    "        \n",
    "        #if i % 2 == 100:#after 100 epoch we save\n",
    "        #if i % sample_period == 0:\n",
    "        print(f\"epoch: {i+1}, d_loss: {d_loss:.2f}, \\\n",
    "        d_acc: {d_acc:.2f}, g_loss: {g_loss:.2f}\")\n",
    "        sample_tweet(i,noise,sampled_labels, generator)\n",
    "        #disc_text(discriminator, tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-concord",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-animal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-garlic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
