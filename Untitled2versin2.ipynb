{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69eafa1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense,Input,ConvLSTM2D,Reshape,Activation,Lambda,Softmax\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, TimeDistributed, Flatten, Dropout,RepeatVector,Reshape,Activation,Lambda,Softmax\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "import keras.backend as kerback\n",
    "#from keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "import tensorflow \n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df90cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a generative adversarial network on a one-dimensional function\n",
    "from numpy import hstack\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import random\n",
    "from numpy.random import rand\n",
    "from numpy.random import randn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd63fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(f):\n",
    "    data = pd.read_csv(f, sep=\"\\n\",header=None)\n",
    "    list_listword=[]#liste des listes des mots de chaque tweet\n",
    "    list_tweets=data[0].values.tolist()#liste des tweets\n",
    "    l=[]\n",
    "    for text in list_tweets:\n",
    "        text = re.sub(r'http\\S+', '', text)   # Remove URLs\n",
    "        text = re.sub(r'—', ' ', text) \n",
    "        text = re.sub(r'@[a-zA-Z0-9_]+', '', text)  # Remove @ mentions\n",
    "        text = text.strip(\" \")   # Remove whitespace resulting from above\n",
    "        text = re.sub(r' +', ' ', text)   # Remove redundant spaces\n",
    "        l.append(text)\n",
    "    \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e26ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254\n"
     ]
    }
   ],
   "source": [
    "list_tweets=prepare_data('C:/Users/takwa/OneDrive/Bureau/tweet.txt')\n",
    "print(len(list_tweets[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bea933b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[146, 7, 46, 5, 2, 147, 148, 106, 3, 17, 149, 107, 1, 348, 349, 30, 350, 58, 351, 352, 3, 353, 354, 208, 52, 355, 356, 357, 7, 358, 3, 209, 47, 18, 359, 7, 360, 22, 4, 27]\n",
      "40\n",
      "57\n"
     ]
    }
   ],
   "source": [
    "def tokenizer_data(list_tweets):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(list_tweets)\n",
    "    sequences = tokenizer.texts_to_sequences(list_tweets)\n",
    "    vocab_size=len(tokenizer.word_index)\n",
    "    return sequences, tokenizer\n",
    "\n",
    "def max_tweet(sequences):\n",
    "    max_length=0\n",
    "    for i in range(len(sequences)):\n",
    "        if(len(sequences[i])>max_length):\n",
    "            max_length=len(sequences[i])\n",
    "        #print(\"max\",max_length)\n",
    "        #print(\"seq=\",sequences)\n",
    "        #print(vocab_size)\n",
    "    return max_length\n",
    "\n",
    "sequences, tokenizer=tokenizer_data(list_tweets)\n",
    "print(sequences[0])\n",
    "print(len(sequences[0])) #nombre d'element de tweet zero\n",
    "nb_tweet=len(sequences)\n",
    "#print(len(tokenizer.word_index))\n",
    "max_length=max_tweet(sequences)#nombre d'element de tout les tweet pour les mettres egaux\n",
    "print(max_length) #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1fae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def build_generator(n_outputs):#max_length nombre d'element das une tweet\n",
    "    model = Sequential()\n",
    "    x0=Input(shape=(n_outputs,1))\n",
    "    x1=LSTM(15, activation='relu',input_dim=n_outputs)(x0)\n",
    "    #15 nombre de neurone ,batch_size=n_batch=20 \n",
    "    # repeat vector\n",
    "    x2=RepeatVector(n_outputs)(x1)\n",
    "    # decoder layer\n",
    "    x3=LSTM(15, activation='relu', return_sequences=True)(x2)\n",
    "    x4=TimeDistributed(Dense(1))(x3)\n",
    "    unstacked = Lambda(lambda x: tensorflow.unstack(x, axis=2))(x4)\n",
    "    dense_outputs = [Dense(n_outputs)(x) for x in unstacked]\n",
    "    merged = Lambda(lambda x: K.stack(x, axis=2))(dense_outputs)\n",
    "\n",
    "    #generated_tweet=model(x0)\n",
    "    model = Model(x0,merged )\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    print('generator')\n",
    "    #print(model.summary())\n",
    "    return  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583f92bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def build_discriminator(n_inputs):\n",
    "    model = Sequential()\n",
    "    x0=Input(shape=(n_inputs,1)) #n_imput nombre des element \n",
    "    #x1=Flatten()(x0)\n",
    "    x2=LSTM(15, activation='relu', input_dim=n_inputs,batch_size=30)(x0)\n",
    "    x3=RepeatVector(n_inputs)(x2)\n",
    "    x4=LSTM(15, activation='relu', return_sequences=True)(x3)\n",
    "    x5=TimeDistributed(Dense(1))(x4)\n",
    "    unstacked = Lambda(lambda x: tensorflow.unstack(x, axis=2))(x5)\n",
    "    dense_outputs = [Dense(n_inputs)(x) for x in unstacked]\n",
    "    merged = Lambda(lambda x: K.stack(x, axis=2))(dense_outputs)\n",
    "    x1=Flatten()(merged)\n",
    "    x6=Dense(1, activation='sigmoid')(x1)\n",
    "    model = Model(x0, x6)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy' ,metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a0e35a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(generator, discriminator):\n",
    "    # make weights in the discriminator not trainable\n",
    "    discriminator.trainable = False\n",
    "    # connect them\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3366eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples():\n",
    "    list_tweets=prepare_data('C:/Users/takwa/OneDrive/Bureau/tweet.txt')\n",
    "    sequences, tokenizer=tokenizer_data(list_tweets)\n",
    "    #print(sequences[0])\n",
    "    #print(\"len=\",len(sequences[0]))\n",
    "    max_length=max_tweet(sequences)\n",
    "    #print(\"max_length\",max_length)\n",
    "    #print(sequences[0].shape)\n",
    "    t =sequences[:50]#on a prix les 30 1er tweets\n",
    "    t = pad_sequences(t, maxlen=max_length, padding='post')\n",
    "    #print(\"len\",len(t[0]))\n",
    "    #print(t[0])\n",
    "    #print(t[0].shape)\n",
    "   \n",
    "    #print(\"len1\",len(sequences))\n",
    "    #print(\"len2\",len(t))\n",
    "    #X=X.reshape(30, 70, 1) #33 array chaque array contient 70 array chaque array contient 1 element\n",
    "    #print(t[0]) #262 elements\n",
    "    #print(t)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5818f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch labels to use when calling train_on_batch\n",
    "y_real = ones((50,1))\n",
    "y_fake = zeros((50,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c5a5e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "functional_3 (Functional)    (None, 57, 1)             6202      \n",
      "_________________________________________________________________\n",
      "functional_1 (Functional)    (None, 1)                 6260      \n",
      "=================================================================\n",
      "Total params: 12,462\n",
      "Trainable params: 6,202\n",
      "Non-trainable params: 6,260\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(max_length)\n",
    "#discriminator.compile(optimizer='adam', loss='binary_crossentropy' ,metrics=['accuracy'])\n",
    "\n",
    "# Build and compile the combined model\n",
    "generator = build_generator(max_length)\n",
    "gan_model = define_gan(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdbcf2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def save_tweet():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7587c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_tweet(epoch,noise,g_model):\n",
    "    result = g_model.predict(noise)\n",
    "    #print(result)\n",
    "    #print(result.shape)\n",
    "    #print(len(result))\n",
    "    res=[]\n",
    "    #for i in range(len(result)):\n",
    "    predicted_word = ''\n",
    "    for j in range(57):\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == int(result[0][j]):\n",
    "                predicted_word = predicted_word+' '+word\n",
    "                break\n",
    "    res.append(predicted_word)\n",
    "    print(res)\n",
    "    \n",
    "    #for i in res:\n",
    "    #    print(i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "731cf26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_text(model_disc, seed_text):\n",
    "    #print(seed_text)\n",
    "    sequences, tokenizer=tokenizer_data(seed_text)\n",
    "    sequences= pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "    #y=model_disc.predict([encoded])\n",
    "    y = model_disc.predict(sequences)\n",
    "    #print(y)\n",
    "    for i in y:\n",
    "        if(i<0.6):\n",
    "            print(\"fake\")\n",
    "        else:\n",
    "            print(\"real\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  1\n",
      "[' to and and the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  2\n",
      "[' to the the to the the and to to the the the the to to the and the the and the the']\n",
      "Epoch  3\n",
      "[' to the the to the the and to to the the the the to to and and and and and the the']\n",
      "Epoch  4\n",
      "[' to and and the and the a to the to the and and the and the to and a and a a and and']\n",
      "Epoch  5\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  6\n",
      "[' to and and the and the a to the to the and and the the the the and a and a a and and']\n",
      "Epoch  7\n",
      "[' the a a and and and a to the to and a a and and and the a in a a in a and']\n",
      "Epoch  8\n",
      "[' and of to of a in in our to a the a of of to a in a and our you our our you of in']\n",
      "Epoch  9\n",
      "[' to the the to the the and to to the the the the to to the and the and and the the']\n",
      "Epoch  10\n",
      "[' the a a and and and a to the to and a a and and and the a in a a in a and']\n",
      "Epoch  11\n",
      "[' to the the the the the and to to the the the the the the to and and and and and and the']\n",
      "Epoch  12\n",
      "[' to the the the the the and to the the the the the the to and and and and and the the']\n",
      "Epoch  13\n",
      "[' to and and the the the and to the to the and and the the the the and a and and a and and']\n",
      "Epoch  14\n",
      "[' a our to our in our our this the in the in you our to of of in a this it this this it you our']\n",
      "Epoch  15\n",
      "[' and in to in a in in of to and to a of in to a in a and of our of of our of in']\n",
      "Epoch  16\n",
      "[' to the the the the the and to to the and the the the the to and and and and and and the']\n",
      "Epoch  17\n",
      "[' to and and the and the and to the to the and and the the the the and a and and a and and']\n",
      "Epoch  18\n",
      "[' a our to our in of of you the a the in you our to in of in a you this you you this you of']\n",
      "Epoch  19\n",
      "[' to the the the the the and to to the the the the the the to and and and and and and the']\n",
      "Epoch  20\n",
      "[' to and and to and the a the the a and the the the to and a and and a and and']\n",
      "Epoch  21\n",
      "[' to and and the the the and to the to the and and the the the the and a and and a and the']\n",
      "Epoch  22\n",
      "[' the and and the and and a to the to the a and the and the the a a a a a a and']\n",
      "Epoch  23\n",
      "[' to the the to the to the to to the the to to to to the the the the the the the']\n",
      "Epoch  24\n",
      "[' to the the the the the and the to the and the the the the to and and and and and and the']\n",
      "Epoch  25\n",
      "[' in this the you of you you for the of and of this this to our you of in it is it it is this you']\n",
      "Epoch  26\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  27\n",
      "[' and our to of in of of you the a the in our our to in of in a you this you you this our of']\n",
      "Epoch  28\n",
      "[' and in to in a in in our to and the a of in to a in a and of our of our our of in']\n",
      "Epoch  29\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  30\n",
      "[' the a a and a and in to and to and in a and a and the in in in in in a a']\n",
      "Epoch  31\n",
      "[' the and and and and and a to the to and a and and and the the a in a a in a and']\n",
      "Epoch  32\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  33\n",
      "[' and of to of a in in our to a the a our of to in in a a our you our our you our in']\n",
      "Epoch  34\n",
      "[' a our to our in of of this the in the in you our to of of in a you this this this it you of']\n",
      "Epoch  35\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  36\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  37\n",
      "[' to to to to to to to to to to to to to to to to to the to to']\n",
      "Epoch  38\n",
      "[' to the the to to to the to to the the to to to to the the the the the the the']\n",
      "Epoch  39\n",
      "[' and in to in a in in our to a the a of of to a in a and of our our our you of in']\n",
      "Epoch  40\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  41\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  42\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  43\n",
      "[' and of to of in of in you to a the in our of to in in a a our you our our you our of']\n",
      "Epoch  44\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  45\n",
      "[' and in to in a in in of to and to a of in to a a a and of our of of our of in']\n",
      "Epoch  46\n",
      "[' the a a and and and in to the to and a a and and and the a in a in in a and']\n",
      "Epoch  47\n",
      "[' to to to to to the to to to to to to to to to the the the the to to']\n",
      "Epoch  48\n",
      "[' and of to of a in in our to a the a of of to a in a and our our our our you of in']\n",
      "Epoch  49\n",
      "[' a our to our in our our this the in the in you our to of our in a this it this this it you our']\n",
      "Epoch  50\n",
      "[' all election in can i make as or in i our we country can the my my all that at but at at one can on and']\n",
      "Epoch  51\n",
      "[' to the to to the to the to to the to to to to the the the the the the the']\n",
      "Epoch  52\n",
      "[' to the the the the the and to the and the the the the to and and and and and and the']\n",
      "Epoch  53\n",
      "[' and in in and of in our to a to a our in a in a the of our in of our of in to']\n",
      "Epoch  54\n",
      "[' to and and the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  55\n",
      "[' and of to of a in in our to a the a of of to a in a and our our our our you of in']\n",
      "Epoch  56\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  57\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  58\n",
      "[' to to to to to the to to the to to to to to the to the the to to']\n",
      "Epoch  59\n",
      "[' to to to to to the to to to to to to to to to the the the the to to']\n",
      "Epoch  60\n",
      "[' to the the to the to the to to the the to the to to the the the the the the the']\n",
      "Epoch  61\n",
      "[' the a a and and and a to the to and a a and and and the a in a a in a and']\n",
      "Epoch  62\n",
      "[' in it the it our this this is the our and our for it to you this our of is vote is is vote for this']\n",
      "Epoch  63\n",
      "[' to and and the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  64\n",
      "[' and in to in a in a of to and to a in in to a a a and of our of of our in a']\n",
      "Epoch  65\n",
      "[' to the the to the the and to the the the the the to to and and and and and the the']\n",
      "Epoch  66\n",
      "[' to and and the and and and to the to the and and the the the the and a and a a and and']\n",
      "Epoch  67\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  68\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  69\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  70\n",
      "[' to and and the and and a to the to the and and the and the the and a a a a and and']\n",
      "Epoch  71\n",
      "[' to the the to the to the to to the to to to to to the the the the the the the']\n",
      "Epoch  72\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  73\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  74\n",
      "[' to to to to to to to to to to to to to to to to to the to to']\n",
      "Epoch  75\n",
      "[' the and and and and and a to the to and a and and and the the a a a a in a and']\n",
      "Epoch  76\n",
      "[' to the the the the the and to the the the the the the to and and and and and and the']\n",
      "Epoch  77\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  78\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  79\n",
      "[' to to to to to to to to to to to to to to the to to the to to']\n",
      "Epoch  80\n",
      "[' in it the it our this this vote the our and you for it to you this our of is all is is all for this']\n",
      "Epoch  81\n",
      "[' a you to you of our our this the in the of you you to of our in in this it this this it you our']\n",
      "Epoch  82\n",
      "[' the a to a and and and in to the to and a a and and and the a in in in in a and']\n",
      "Epoch  83\n",
      "[' and of to in a in in our to a the a of of to a in a and our our our our you of in']\n",
      "Epoch  84\n",
      "[' to and and the and the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  85\n",
      "[' to the the the the the and to the and the the the the to and and and and and the the']\n",
      "Epoch  86\n",
      "[' to to to to to to the to to the the to to to to the the the the the the to']\n",
      "Epoch  87\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  88\n",
      "[' to and and the and and a to the to the and and the and the the and a a a a and and']\n",
      "Epoch  89\n",
      "[' to the the to the the the to to the the to the to to the the the the and the the']\n",
      "Epoch  90\n",
      "[' the a to a and a a in to and to and a a and a and the in in in in of in a']\n",
      "Epoch  91\n",
      "[' the and and the and and a to the to the a and and and the the a a a a in a and']\n",
      "Epoch  92\n",
      "[' the a to a and a a in to and to and a a and a and the in in in in of in a']\n",
      "Epoch  93\n",
      "[' the a to a and a and in to the to and a a and and and the in in in in in a a']\n",
      "Epoch  94\n",
      "[' to the the to the the and to to the the the the to to the and the and and the the']\n",
      "Epoch  95\n",
      "[' and of to of in of of you the a the in our of to in of in a our you you you this our of']\n",
      "Epoch  96\n",
      "[' the and and and and and a to the to and a and and and the the a in a a in a and']\n",
      "Epoch  97\n",
      "[' to the the to the the and to to the the the the to to the and the and and the the']\n",
      "Epoch  98\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  99\n",
      "[' to to to to to to to to to to to to to to the to to the to to']\n",
      "Epoch  100\n",
      "[' to to to to to to to to to to to to to to the to to the to to']\n",
      "Epoch  101\n",
      "[' and of to of in of of you the a the in our of to in of in a you this you you this our of']\n",
      "Epoch  102\n",
      "[' to the the the the the and the to the and and the the the to and and and and a and the']\n",
      "Epoch  103\n",
      "[' the a to a and a a in to and to and in a to and a and the in of in in of in a']\n",
      "Epoch  104\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  105\n",
      "[' the and and the and and a to the to the and and the and the the a a a a a a and']\n",
      "Epoch  106\n",
      "[' the in to in and a a of to and to and in in to a a and and in of of of of in a']\n",
      "Epoch  107\n",
      "[' to to to the to the to to the to to to to the the to the the to to']\n",
      "Epoch  108\n",
      "[' to to to to to to to to to to to to to to to the to to the to to']\n",
      "Epoch  109\n",
      "[' the a a and and and in to the to and a a and and and the a in in in in a and']\n",
      "Epoch  110\n",
      "[' the a to a and a a in to and to and a a and a and the in in in in of in a']\n",
      "Epoch  111\n",
      "[' in this the this our you you for the of and our it this to our you of in it is it for is it you']\n",
      "Epoch  112\n",
      "[' the a to a and a a in to and to and in a and a and the in of in in of in a']\n",
      "Epoch  113\n",
      "[' the and and and and and a to the to and a and and and and the a in a a in a and']\n",
      "Epoch  114\n",
      "[' to and and the and and a to the to the and and the and the the and a a a a and and']\n",
      "Epoch  115\n",
      "[' to and and the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  116\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  117\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  118\n",
      "[' to the the to the the the to to the the to the to to the the the the and the the']\n",
      "Epoch  119\n",
      "[' to the the to to to the to to the the to to to to the the the the the the the']\n",
      "Epoch  120\n",
      "[' to to to to to to the to to the to to to to the the the the the to to']\n",
      "Epoch  121\n",
      "[' the a to a and a a in to and to and in a and a and the in of in in of in a']\n",
      "Epoch  122\n",
      "[' to to to to to to to to to to to to to to to']\n",
      "Epoch  123\n",
      "[' to the the to the the and to to the the the the to to the and and and and the the']\n",
      "Epoch  124\n",
      "[' to the the the the the and to the and the the the the to and and and and and and the']\n",
      "Epoch  125\n",
      "[' the and and the and and a to the to the a and and and the the a a a a in a and']\n",
      "Epoch  126\n",
      "[' in this to you of you you for the of and of this this to our you of in it for it it is this you']\n",
      "Epoch  127\n",
      "[' to the the the the the and to the the the the the to to and and and and and the the']\n",
      "Epoch  128\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  129\n",
      "[' to the the the the the and to to the and the the the the to and and and and and and the']\n",
      "Epoch  130\n",
      "[' the a a and a and in to the to and a a and and and the in in in in in a a']\n",
      "Epoch  131\n",
      "[' to to to to to to to to to to to to to to the to to the to to']\n",
      "Epoch  132\n",
      "[' and of to of in of of you the a the in our of to in of in a you you you you this our of']\n",
      "Epoch  133\n",
      "[' the a to a and a a in to and to and in a and a and the in of in in of in a']\n",
      "Epoch  134\n",
      "[' the a to a and a a in to and to and a a and a and the in in in in in a a']\n",
      "Epoch  135\n",
      "[' and of to of in of of you the a the in our of to in in a a our you our our you our of']\n",
      "Epoch  136\n",
      "[' to the the the the the and the to the and and the the the to and and and and a and the']\n",
      "Epoch  137\n",
      "[' to the the to the the and to to the the the the to to the and and and and the the']\n",
      "Epoch  138\n",
      "[' to and and the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  139\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  140\n",
      "[' to the the to the the and to to the the the the to to the and and and and the the']\n",
      "Epoch  141\n",
      "[' to and and the and the and to the to the and and the the the to and a and and a and and']\n",
      "Epoch  142\n",
      "[' to the the the the the and to to the and the the the the to and and and and and and the']\n",
      "Epoch  143\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  144\n",
      "[' to the the the the the and to the and the the the the to and and and and and and the']\n",
      "Epoch  145\n",
      "[' to the the the the the and to the to the and the the the the to and and and and and and the']\n",
      "Epoch  146\n",
      "[' to and and the and the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  147\n",
      "[' and our to of in of of you the a the in our our to in of in a you this you you this our of']\n",
      "Epoch  148\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  149\n",
      "[' to the the the the the and to to the and the the the the to and and and and and and the']\n",
      "Epoch  150\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  151\n",
      "[' to the the to the the the to to the the to the to to the the the the and the the']\n",
      "Epoch  152\n",
      "[' the a to a and a a in to and to and in a and a and the in of in in of in a']\n",
      "Epoch  153\n",
      "[' and of to of in of of you the a the in our of to in of in a you you you you this our of']\n",
      "Epoch  154\n",
      "[' the and and the and and a to the to the a and the and the the a a a a in a and']\n",
      "Epoch  155\n",
      "[' to to to to to to to to to to to to to to to to']\n",
      "Epoch  156\n",
      "[' to the the to the to the to to the the to the to to the the the the the the the']\n",
      "Epoch  157\n",
      "[' the a to a and a a in to and to and in a and a and the in of in in of in a']\n",
      "Epoch  158\n",
      "[' to and and the and the and to the to the and and the the the the and a and and a and and']\n",
      "Epoch  159\n",
      "[' to to to to to the to to to to to to to to to the to the the to to']\n",
      "Epoch  160\n",
      "[' the a to a and a a in to and to and in a and a and the in in in in of in a']\n",
      "Epoch  161\n",
      "[' the a a and a and in to and to and in a and a and the in in in in in a a']\n",
      "Epoch  162\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  163\n",
      "[' the and and and and and a to the to and a and and and the the a in a a in a and']\n",
      "Epoch  164\n",
      "[' and in to in a in in of to and the a of in to a in a and of our of of our of in']\n",
      "Epoch  165\n",
      "[' the and and and and and a to the to and a a and and and the a in a a in a and']\n",
      "Epoch  166\n",
      "[' to the the to the to the to to the the to to to to the the the the the the the']\n",
      "Epoch  167\n",
      "['']\n",
      "Epoch  168\n",
      "[' to and and the and and a to the to the and and the and the the and a a a a and and']\n",
      "Epoch  169\n",
      "[' to and and the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  170\n",
      "[' to to to to to the to to the to to to to to the the the the the to to']\n",
      "Epoch  171\n",
      "[' the and and the and and a to the to the a and the and the the a a a a a a and']\n",
      "Epoch  172\n",
      "[' a you to you of our our this the in the of you you to of our in in this it this this it you our']\n",
      "Epoch  173\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  174\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  175\n",
      "[' to the the to to to the to to the the to to to to the the the the the the the']\n",
      "Epoch  176\n",
      "[' of for the it you this this vote and our and you for it to you this our of is all is is all for this']\n",
      "Epoch  177\n",
      "[' to and and the and the and to the to the and and the the the the and a and and a and and']\n",
      "Epoch  178\n",
      "[' to and and the and the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  179\n",
      "[' to to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  180\n",
      "[' the in to in a a a of to and to a in in to a a and and of of of of our in a']\n",
      "Epoch  181\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  182\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  183\n",
      "[' the in to in a a a of to and to a in in to a a and and of of of of our in a']\n",
      "Epoch  184\n",
      "[' the and and and and and a to the to and a a and and and the a in a a in a and']\n",
      "Epoch  185\n",
      "[' the in to in and a a of to and to and in in to a a and and in of of of of in a']\n",
      "Epoch  186\n",
      "[' to to to to to to to to to to to to to to to to to to to']\n",
      "Epoch  187\n",
      "[' the in to in a a a of to and to a in in to a a and and of of of of our in a']\n",
      "Epoch  188\n",
      "[' to the the the the the and to to the and the the the the to and and and and and and the']\n",
      "Epoch  189\n",
      "[' to the the to the the and to to the the the the to to the and the and and the the']\n",
      "Epoch  190\n",
      "[' our vote the vote this for for your and this a this all vote to it for you you that i that that i vote for to']\n",
      "Epoch  191\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  192\n",
      "[' to to to to to to the to to the to to to to to the the the the the to to']\n",
      "Epoch  193\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  194\n",
      "[' the a to a and a a in to and to and in a and a and the in of in in of in a']\n",
      "Epoch  195\n",
      "[' to the the the the the and to the the the the the to to and and and and and the the']\n",
      "Epoch  196\n",
      "[' to and and the the the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  197\n",
      "[' to the the the the the and to the the the the the to to the and and and and the the']\n",
      "Epoch  198\n",
      "[' the a a and and and in to the to and a a and and and the a in a in in a and']\n",
      "Epoch  199\n",
      "[' to the the to the to the to to the the to the to to the the the the the the the']\n",
      "Epoch  200\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  201\n",
      "[' to the the the the the and to the the the the the the to and and and and and the the']\n",
      "Epoch  202\n",
      "[' a our to our in of of you the a the in our our to in of in a you this you you this our of']\n",
      "Epoch  203\n",
      "[' to the the to the the the to to the the the the to to the and the the and the the']\n",
      "Epoch  204\n",
      "[' and in to in a in in of to and the a of in to a in a and of our of of our of in']\n",
      "Epoch  205\n",
      "[' to to to to to to to to to to to to to to to to to the to to']\n",
      "Epoch  206\n",
      "[' and in to in a in a of to and to a of in a a a and of our of of our in in']\n",
      "Epoch  207\n",
      "[' to and and the the the and to the to the and and the the the the and a and and a and the']\n",
      "Epoch  208\n",
      "[' can time you get was or right because you from is do we’re day the us will can election we’re his we’re how because get but of to']\n",
      "Epoch  209\n",
      "[' the and and the and and a to the to and a and and and the the a in a a in a and']\n",
      "Epoch  210\n",
      "[' to and and the and the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  211\n",
      "[' to the the the the the and to the the the the the the to and and and and and and the']\n",
      "Epoch  212\n",
      "[' to the the the the the and to to the the the the the the to and and and and and and the']\n",
      "Epoch  213\n",
      "[' the a to a and a and in to the to and a a and and and the in in in in in a a']\n",
      "Epoch  214\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  215\n",
      "[' to the the to the to the to to the the to to to to the the the the the the the']\n",
      "Epoch  216\n",
      "[' our vote the is this for for your and you a this all is to it for you you that i that that i vote for to']\n",
      "Epoch  217\n",
      "[' the and and and and and a to the to and a a and and and the a in a a in a and']\n",
      "Epoch  218\n",
      "[' to and and the the the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  219\n",
      "[' to to to to to the to to to to to to to to the the the the the to to']\n",
      "Epoch  220\n",
      "[' the a a the a and in to the to and a a and and and the a in a a in a a']\n",
      "Epoch  221\n",
      "[' and in to in a in in our to and the a of in to a in a and of our of of our of in']\n",
      "Epoch  222\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  223\n",
      "[' to to to to to to to to to to to to to to']\n",
      "Epoch  224\n",
      "[' to and and the and and a to the to the and and the and the the a a a a a and and']\n",
      "Epoch  225\n",
      "[' to to to to to to to to to to to to to to to to']\n",
      "Epoch  226\n",
      "[' to and and the and the and to the to the and and the the the the and a and and a and and']\n",
      "Epoch  227\n",
      "[' to to to to the to the to to the to to to to to the the the the the the the']\n",
      "Epoch  228\n",
      "[' to to to to to the to to to to to to to to the the the the the to to']\n",
      "Epoch  229\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  230\n",
      "[' and in to in a in in of to and to a of in to a a a and of our of of our of in']\n",
      "Epoch  231\n",
      "[' to the to to to to the to to the the to to to to the the the the the the to']\n",
      "Epoch  232\n",
      "[' the a to a and and and in to the to and a a and and and the in in in in in a and']\n",
      "Epoch  233\n",
      "[' the a to a and a and in to the to and a a and and and the in in in in in a and']\n",
      "Epoch  234\n",
      "[' to to to to to to to to to to to to to to to to']\n",
      "Epoch  235\n",
      "[' the in to a and a a of to and to and in in a a and and in of in of of in a']\n",
      "Epoch  236\n",
      "[' of it the it our this this vote and our and you for it to you this our of is all is is all for this']\n",
      "Epoch  237\n",
      "[' to to to the to the to to the to to to to to the to the the to to']\n",
      "Epoch  238\n",
      "[' the a to a and a a in to and to and a a and a and the in in in in of in a']\n",
      "Epoch  239\n",
      "[' to the the to the the the to to the the to the to to the and the the and the the']\n",
      "Epoch  240\n",
      "[' the a to a and a and in to the to and a a and and and the in in in in in a a']\n",
      "Epoch  241\n",
      "[' the a to a and a and in to the to and a a and and and the in in in in in a a']\n",
      "Epoch  242\n",
      "[' to and and the and the and to the to the and and the the the to and a and and a and and']\n",
      "Epoch  243\n",
      "[' the a to a and a a in to the to and a a and and and the in in in in in a a']\n",
      "Epoch  244\n",
      "[' to the the to the the and to to and the the the to to the and the the and the the']\n",
      "Epoch  245\n",
      "[' the in to in a in a of to and to a in in to a a and and of of of of our in a']\n",
      "Epoch  246\n",
      "[' to the to to to to the to to the the to to to to the the the the the the to']\n",
      "Epoch  247\n",
      "[' to to to to to to to to to to to to to to to to']\n",
      "Epoch  248\n",
      "[' the and and the and and a to the to the a and and and the the a a a a in a and']\n",
      "Epoch  249\n",
      "[' the and and the and and a to the to the and and the and the the a a a a a a and']\n",
      "Epoch  250\n",
      "[' the and and the and and a to the to the a and and and the the a a a a a a and']\n",
      "Epoch  251\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  252\n",
      "[' to to to to to to the to to the to to to to to the the the the the the to']\n",
      "Epoch  253\n",
      "[' and of to of in of of you the a the in our of to in of in a our you you you this our of']\n",
      "Epoch  254\n",
      "[' the a to a and a a in to the to and a a and a and the in in in in of a a']\n",
      "Epoch  255\n",
      "[' to the the the the the and to the to the and and the the the to and and and and a and the']\n",
      "Epoch  256\n",
      "[' to the the the the the and to to the the the the the the to and and and and and and the']\n",
      "Epoch  257\n",
      "[' to to to to to to to to to to to to to to the to to the to to']\n",
      "Epoch  258\n",
      "[' the and and the and and a to the to the a and the and the the a a a a a and and']\n",
      "Epoch  259\n",
      "[' the in to in a in a of to and to a in in to a a a and of of of of our in a']\n",
      "Epoch  260\n",
      "[' to and and the the the and to the to the and and the the the to and a and and a and the']\n",
      "Epoch  261\n",
      "[' and of to of a of in our to a the a our of to in in a and our you our our you our in']\n",
      "Epoch  262\n",
      "[' to to to to to to to to to to to to to to to to to to']\n"
     ]
    }
   ],
   "source": [
    "sample_period = 100 # every `sample_period` steps generate and save some data\"\n",
    "Epoch=3000\n",
    "for i in range(Epoch):\n",
    "        # prepare real samples\n",
    "        x_real= generate_real_samples()\n",
    "\n",
    "        noise=random.randn(50,1)\n",
    "        x_fake= generator.predict(noise)\n",
    "        #print(\"x_fake shape\",x_fake.shape)\n",
    "        \n",
    "        # update discriminator\n",
    "        discriminator_metric_real=discriminator.train_on_batch(x_real, y_real)\n",
    "        discriminator_metric_genereted =discriminator.train_on_batch(x_fake, y_fake)\n",
    "        discriminator_metric=0.5 * np.add(discriminator_metric_real,discriminator_metric_genereted)\n",
    "        \n",
    "        # create inverted labels for the fake samples\n",
    "        y_gan = ones((50, 1))\n",
    "       # print(\"gan\")\n",
    "        #print(x_gan.shape,y_gan.shape)\n",
    "        # update the generator via the discriminator's error\n",
    "        noise=random.randn(50,1)\n",
    "        generator_metric=gan_model.train_on_batch(noise, y_gan)\n",
    "        \n",
    "         # evaluate discriminator on real examples\n",
    "        acc_real = discriminator.evaluate(x_real, y_real, verbose=0)\n",
    "        # evaluate discriminator on fake examples\n",
    "        acc_fake = discriminator.evaluate(x_fake, y_fake, verbose=0)\n",
    "        \n",
    "        #disc_text(discriminator,generate_tweets)\n",
    "        #if i % 2 == 100:#after 100 epoch we save\n",
    "         #   print(\"Epoch \",i,\"Discriminator accuarcy :\", discriminator_metric[1] ,\"Generator accuarcy :\")\n",
    "        #print('summarize discriminator performance :')\n",
    "        #print(\"Epoch \",i, acc_real, acc_fake)\n",
    "        #if i % sample_period == 0:\n",
    "        print(\"Epoch \",i)\n",
    "        noiseg=random.randn(1,57)\n",
    "        sample_tweet(i,noiseg, generator)\n",
    "            #disc_text(discriminator, tex)\n",
    "            #, discriminator_metric[1],\"Generator accuarcy :\",generator_metric[1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b5e9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
